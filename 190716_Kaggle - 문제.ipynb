{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29400, 784)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(29400, 10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "0.20917295\n",
      "0.009655466\n",
      "0.00070271245\n",
      "0.00014997795\n",
      "1.5719643e-05\n",
      "2.9967466e-06\n",
      "6.055769e-07\n",
      "1.1920907e-07\n",
      "2.0265574e-08\n",
      "4.7683715e-09\n",
      "정확도:0.9769859313964844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 0, 9, ..., 3, 9, 2], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Kaggle - Digit Recognizer 문제 풀기\n",
    "#경로: https://www.kaggle.com/c/digit-recognizer\n",
    "#train.csv =>7:3 비율로 나누어 학습 및 accuracy 확인\n",
    "# - batch형태로 데이터를 읽어들여서 학습\n",
    "\n",
    "#test.csv 를 이용하여 prediction 결과 도출\n",
    "#해당 결과를 이용하여 submission.csv파일을 생성한 후 kaggle site 에 제출 및 확인\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "tf.reset_default_graph()\n",
    "df = pd.read_csv(\"./data/recognizer/train.csv\",sep=\",\",skiprows=0)\n",
    "\n",
    "split_cnt = int(df.shape[0] * 0.7)\n",
    "train_df = df.loc[:split_cnt,:]\n",
    "test_df = df.loc[split_cnt:,:]\n",
    "\n",
    "x_data = MinMaxScaler().fit_transform(train_df.loc[:,\"pixel0\":])\n",
    "y_data = tf.one_hot(train_df[\"label\"],10) \n",
    "sess = tf.Session()\n",
    "y_data = sess.run(y_data)\n",
    "\n",
    "x_data_test = MinMaxScaler().fit_transform(test_df.loc[:,\"pixel0\":])\n",
    "y_data_test = tf.one_hot(test_df[\"label\"],10) \n",
    "y_data_test = sess.run(y_data_test)\n",
    "\n",
    "display(x_data.shape) #(42000, 784)\n",
    "display(y_data.shape) #(42000, 1)\n",
    "\n",
    "print(type(x_data))\n",
    "print(type(y_data))\n",
    "#placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32) \n",
    "Y = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "\n",
    "keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "#Weight & bias\n",
    "#행렬곱 연산을 해야하기때문에 [784,10]\n",
    "#W1 = tf.Variable(tf.random_normal([784,300]), name=\"weight1\") \n",
    "W1 = tf.get_variable(\"weight1\", shape=[784,256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]), name=\"bias1\")\n",
    "_layer1 = tf.nn.relu(tf.matmul(X,W1) +  b1)\n",
    "layer1 = tf.nn.dropout(_layer1, keep_prob=keep_prob)\n",
    "\n",
    "#W2 = tf.Variable(tf.random_normal([300,300]), name=\"weight2\") \n",
    "W2 = tf.get_variable(\"weight2\", shape=[256,256],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]), name=\"bias2\")\n",
    "_layer2 = tf.nn.relu(tf.matmul(layer1,W2) +  b2)\n",
    "layer2 = tf.nn.dropout(_layer2, keep_prob=keep_prob)\n",
    "\n",
    "#W3 = tf.Variable(tf.random_normal([300,10]), name=\"weight3\") \n",
    "W3 = tf.get_variable(\"weight3\", shape=[256,10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]), name=\"bias3\")\n",
    "\n",
    "#Hypothesis\n",
    "logits = tf.matmul(layer2,W3) + b3 #행열곱\n",
    "#H = tf.nn.softmax(logits) #확율값 높은값 추출\n",
    "H = tf.nn.relu(logits)\n",
    "\n",
    "#cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "\n",
    "#training node생성\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "#session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습진행\n",
    "training_epoch = 100 \n",
    "batch_size = 100 \n",
    "                 \n",
    "for step in range(training_epoch):\n",
    "    num_of_iter = int(x_data.shape[0] / batch_size)\n",
    "    cost_val = 0\n",
    "    start_idx = 0\n",
    "    end_idx = 0\n",
    "    for i in range(num_of_iter):\n",
    "        start_idx = (i * training_epoch)\n",
    "        end_idx = start_idx + training_epoch\n",
    "        if end_idx > x_data.shape[0]:\n",
    "            end_idx = x_data.shape[0]\n",
    "                \n",
    "        batch_x = x_data[start_idx:end_idx,:]\n",
    "        batch_y = y_data[start_idx:end_idx,:]\n",
    "        \n",
    "        _, cost_val = sess.run([train,cost],feed_dict={X:batch_x,Y:batch_y,keep_prob:1.0})\n",
    "        \n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(cost_val)\n",
    "        \n",
    "\n",
    "#Accuracy 정확도 측정\n",
    "predict = tf.argmax(H,1) #열방향으로 큰값 추출\n",
    "correct = tf.equal(predict, tf.argmax(Y,1)) #실제값과 예측값 비교\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,dtype=tf.float32))\n",
    "\n",
    "result = sess.run(accuracy,feed_dict={X:x_data_test,\n",
    "                                     Y:y_data_test,keep_prob:1})\n",
    "\n",
    "print(\"정확도:{}\".format(result))\n",
    "\n",
    "#predict check\n",
    "df_test = pd.read_csv(\"./data/recognizer/test.csv\",sep=\",\",skiprows=0)\n",
    "test_x_data = MinMaxScaler().fit_transform(df_test)\n",
    "\n",
    "predict = tf.argmax(H,1)\n",
    "result = sess.run(predict, feed_dict={X:test_x_data, keep_prob: 1.0})\n",
    "df = pd.DataFrame({\n",
    "    'ImageId': [i for i in range(1,28001)],\n",
    "    'Label': result\n",
    "})\n",
    "#display(result)\n",
    "df.to_csv('./data/recognizer/submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape[0]\n",
    "\n",
    "29400 // 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Age2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5</td>\n",
       "      <td>31.2750</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Survived  Pclass Sex   Age  Parch     Fare  Age2\n",
       "0          0       3   1  22.0      0   7.2500  20.0\n",
       "1          1       1   2  38.0      0  71.2833  30.0\n",
       "2          1       3   2  26.0      0   7.9250  20.0\n",
       "3          1       1   2  35.0      0  53.1000  30.0\n",
       "4          0       3   1  35.0      0   8.0500  30.0\n",
       "5          0       3   1   NaN      0   8.4583   NaN\n",
       "6          0       1   1  54.0      0  51.8625  50.0\n",
       "7          0       3   1   2.0      1  21.0750   0.0\n",
       "8          1       3   2  27.0      2  11.1333  20.0\n",
       "9          1       2   2  14.0      0  30.0708  10.0\n",
       "10         1       3   2   4.0      1  16.7000   0.0\n",
       "11         1       1   2  58.0      0  26.5500  50.0\n",
       "12         0       3   1  20.0      0   8.0500  20.0\n",
       "13         0       3   1  39.0      5  31.2750  30.0\n",
       "14         0       3   2  14.0      0   7.8542  10.0\n",
       "15         1       2   2  55.0      0  16.0000  50.0\n",
       "16         0       3   1   2.0      1  29.1250   0.0\n",
       "17         1       2   1   NaN      0  13.0000   NaN\n",
       "18         0       3   2  31.0      0  18.0000  30.0\n",
       "19         1       3   2   NaN      0   7.2250   NaN\n",
       "20         0       2   1  35.0      0  26.0000  30.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data type:<class 'numpy.ndarray'>\n",
      "y_data type:<class 'numpy.ndarray'>\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "정확도:1.0\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "    PassengerId  Survived\n",
      "0           892         0\n",
      "1           893         0\n",
      "2           894         0\n",
      "3           895         0\n",
      "4           896         0\n",
      "5           897         0\n",
      "6           898         0\n",
      "7           899         0\n",
      "8           900         0\n",
      "9           901         0\n",
      "10          902         0\n",
      "11          903         0\n",
      "12          904         0\n",
      "13          905         0\n",
      "14          906         0\n",
      "15          907         0\n",
      "16          908         0\n",
      "17          909         0\n",
      "18          910         0\n",
      "19          911         0\n",
      "20          912         0\n",
      "21          913         0\n",
      "22          914         0\n",
      "23          915         0\n",
      "24          916         0\n",
      "25          917         0\n",
      "26          918         0\n",
      "27          919         0\n",
      "28          920         0\n",
      "29          921         0\n",
      "..          ...       ...\n",
      "388        1280         0\n",
      "389        1281         0\n",
      "390        1282         0\n",
      "391        1283         0\n",
      "392        1284         0\n",
      "393        1285         0\n",
      "394        1286         0\n",
      "395        1287         0\n",
      "396        1288         0\n",
      "397        1289         0\n",
      "398        1290         0\n",
      "399        1291         0\n",
      "400        1292         0\n",
      "401        1293         0\n",
      "402        1294         0\n",
      "403        1295         0\n",
      "404        1296         0\n",
      "405        1297         0\n",
      "406        1298         0\n",
      "407        1299         0\n",
      "408        1300         0\n",
      "409        1301         0\n",
      "410        1302         0\n",
      "411        1303         0\n",
      "412        1304         0\n",
      "413        1305         0\n",
      "414        1306         0\n",
      "415        1307         0\n",
      "416        1308         0\n",
      "417        1309         0\n",
      "\n",
      "[418 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\ipykernel_launcher.py:113: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Titanic: Machine Learning from Disaster 문제 풀기\n",
    "# https://www.kaggle.com/c/titanic/data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "tf.reset_default_graph()\n",
    "df = pd.read_csv(\"./data/titanic/train.csv\",sep=\",\",skiprows=0)\n",
    "\n",
    "#display(df)\n",
    "\n",
    "df2 = df[[\"Survived\",\"Pclass\",\"Sex\",\"Age\",\"Parch\",\"Fare\"]]\n",
    "#df2.dropna(how=\"any\", inplace=True)\n",
    "df2[\"Sex\"] = df2['Sex'].apply(lambda x: '1' if x == 'male' else '2')\n",
    "#df2[\"Age2\"] = df2['Age'].apply(lambda x: 0 if (x is None or x == 'NaN') else (x // 10) * 10)\n",
    "#df2[\"Age2\"] = df2['Age'].apply(lambda x: 0 if (x is None or x == 'NaN') else (x if x < 10 else (x // 10) * 10)\n",
    "display(df2.loc[:20,:])\n",
    "\n",
    "split_cnt = int(df2.shape[0] * 0.7)\n",
    "train_df = df2.loc[:split_cnt,:]\n",
    "test_df = df2.loc[split_cnt:,:]\n",
    "\n",
    "df_x = train_df[[\"Pclass\",\"Sex\",\"Age\",\"Parch\",\"Fare\"]]\n",
    "df_y = train_df[[\"Survived\"]]\n",
    "\n",
    "x_data = MinMaxScaler().fit_transform(df_x)\n",
    "y_data = df_y.values\n",
    "\n",
    "print(\"x_data type:{}\".format(type(x_data)))\n",
    "print(\"y_data type:{}\".format(type(y_data)))\n",
    "\n",
    "#placeholder\n",
    "X = tf.placeholder(shape=[None,5],dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1],dtype=tf.float32)\n",
    "keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "#Weight & bias\n",
    "W1 = tf.get_variable(\"weight1\", shape=[5,20],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([20]), name=\"bias1\")\n",
    "_layer1 = tf.nn.relu(tf.matmul(X,W1) +  b1)\n",
    "layer1 = tf.nn.dropout(_layer1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"weight2\", shape=[20,20],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([20]), name=\"bias2\")\n",
    "_layer2 = tf.nn.relu(tf.matmul(layer1,W2) +  b2)\n",
    "layer2 = tf.nn.dropout(_layer2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"weight3\", shape=[20,1],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([1]), name=\"bias3\")\n",
    "\n",
    "#Hypothesis\n",
    "logits = tf.matmul(layer2,W3) + b3 #행열곱\n",
    "H = tf.nn.relu(logits)\n",
    "\n",
    "#cost function\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "\n",
    "#training node 생성\n",
    "#train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "#session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#학습과정 진행\n",
    "training_epoch = 100 \n",
    "batch_size = 100 \n",
    "                 \n",
    "for step in range(training_epoch):\n",
    "    num_of_iter = int(x_data.shape[0] / batch_size)\n",
    "    cost_val = 0\n",
    "    start_idx = 0\n",
    "    end_idx = 0\n",
    "    for i in range(num_of_iter):\n",
    "        start_idx = (i * training_epoch)\n",
    "        end_idx = start_idx + training_epoch\n",
    "        if end_idx > x_data.shape[0]:\n",
    "            end_idx = x_data.shape[0]\n",
    "                \n",
    "        batch_x = x_data[start_idx:end_idx,:]\n",
    "        batch_y = y_data[start_idx:end_idx,:]\n",
    "        \n",
    "        _, cost_val = sess.run([train,cost],feed_dict={X:batch_x,Y:batch_y,keep_prob:1.0})\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(cost_val)\n",
    "\n",
    "#Accuracy (정확도 측정)\n",
    "df_x_test = test_df[[\"Pclass\",\"Sex\",\"Age\",\"Parch\",\"Fare\"]]\n",
    "df_y_test = test_df[[\"Survived\"]]\n",
    "\n",
    "#Accuracy 정확도 측정\n",
    "predict = tf.argmax(H,1) #열방향으로 큰값 추출\n",
    "correct = tf.equal(predict, tf.argmax(Y,1)) #실제값과 예측값 비교\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,dtype=tf.float32))\n",
    "\n",
    "x_data_t = MinMaxScaler().fit_transform(df_x_test.values)\n",
    "y_data_t = df_y_test \n",
    "\n",
    "result = sess.run(accuracy,feed_dict={X:x_data_t,\n",
    "                                     Y:y_data_t,keep_prob:1})\n",
    "\n",
    "print(\"정확도:{}\".format(result))\n",
    "\n",
    "#===============================================================\n",
    "#predict check\n",
    "df_t = pd.read_csv(\"./data/titanic/test.csv\",sep=\",\",skiprows=0)\n",
    "\n",
    "df_t2 = df_t[[\"PassengerId\",\"Pclass\",\"Sex\",\"Age\",\"Parch\",\"Fare\"]]\n",
    "#df_t2.dropna(how=\"any\", inplace=True)\n",
    "df_t2[\"Sex\"] = df_t2['Sex'].apply(lambda x: '1' if x == 'male' else '2')\n",
    "\n",
    "df_ids = df_t2[[\"PassengerId\"]]\n",
    "df_test = df_t2[[\"Pclass\",\"Sex\",\"Age\",\"Parch\",\"Fare\"]]\n",
    "#print(df_t2)\n",
    "test_x_data = MinMaxScaler().fit_transform(df_test.values)\n",
    "#print(test_x_data)\n",
    "\n",
    "#predict = tf.argmax(H,1)\n",
    "predict = tf.cast(H > 0.5, dtype=tf.float32) #예측값\n",
    "result = sess.run(predict, feed_dict={X:test_x_data, keep_prob: 1.0})\n",
    "#print(type(result))\n",
    "print(result)\n",
    "df = pd.DataFrame({\n",
    "    'PassengerId': [str(i[0]) for i in df_ids.values],\n",
    "    'Survived': [int(i[0]) for i in result]\n",
    "})\n",
    "print(df)\n",
    "\n",
    "df.to_csv('./data/titanic/gender_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "23 // 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Titanic: Machine Learning from Disaster 문제 풀기\n",
    "# https://www.kaggle.com/c/titanic/data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "tf.reset_default_graph()\n",
    "df = pd.read_csv(\"./data/titanic/train.csv\",sep=\",\",skiprows=0)\n",
    "\n",
    "#display(df)\n",
    "\n",
    "df2 = df[[\"Survived\",\"Pclass\",\"Sex\",\"Age\",\"Parch\",\"Fare\"]]\n",
    "#df2.dropna(how=\"any\", inplace=True)\n",
    "df2[\"Sex\"] = df2['Sex'].apply(lambda x: '1' if x == 'male' else '2')\n",
    "#df2[\"Age2\"] = df2['Age'].apply(lambda x: 0 if (x is None or x == 'NaN') else (x // 10) * 10)\n",
    "display(df2.loc[:20,:])\n",
    "\n",
    "split_cnt = int(df2.shape[0] * 0.7)\n",
    "train_df = df2.loc[:split_cnt,:]\n",
    "test_df = df2.loc[split_cnt:,:]\n",
    "\n",
    "df_x = train_df[[\"Pclass\",\"Sex\",\"Age\",\"Parch\",\"Fare\"]]\n",
    "df_y = train_df[[\"Survived\"]]\n",
    "\n",
    "x_data = MinMaxScaler().fit_transform(df_x)\n",
    "y_data = df_y.values\n",
    "\n",
    "print(\"x_data type:{}\".format(type(x_data)))\n",
    "print(\"y_data type:{}\".format(type(y_data)))\n",
    "\n",
    "#placeholder\n",
    "X = tf.placeholder(shape=[None,5],dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1],dtype=tf.float32)\n",
    "keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "#Weight & bias\n",
    "W1 = tf.get_variable(\"weight1\", shape=[5,20],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([20]), name=\"bias1\")\n",
    "_layer1 = tf.nn.relu(tf.matmul(X,W1) +  b1)\n",
    "layer1 = tf.nn.dropout(_layer1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"weight2\", shape=[20,20],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([20]), name=\"bias2\")\n",
    "_layer2 = tf.nn.relu(tf.matmul(layer1,W2) +  b2)\n",
    "layer2 = tf.nn.dropout(_layer2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"weight3\", shape=[20,1],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([1]), name=\"bias3\")\n",
    "\n",
    "#Hypothesis\n",
    "logits = tf.matmul(layer2,W3) + b3 #행열곱\n",
    "H = tf.nn.relu(logits)\n",
    "\n",
    "#cost function\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "\n",
    "#training node 생성\n",
    "#train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "#session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#학습과정 진행\n",
    "training_epoch = 100 \n",
    "batch_size = 100 \n",
    "                 \n",
    "for step in range(training_epoch):\n",
    "    num_of_iter = int(x_data.shape[0] / batch_size)\n",
    "    cost_val = 0\n",
    "    start_idx = 0\n",
    "    end_idx = 0\n",
    "    for i in range(num_of_iter):\n",
    "        start_idx = (i * training_epoch)\n",
    "        end_idx = start_idx + training_epoch\n",
    "        if end_idx > x_data.shape[0]:\n",
    "            end_idx = x_data.shape[0]\n",
    "                \n",
    "        batch_x = x_data[start_idx:end_idx,:]\n",
    "        batch_y = y_data[start_idx:end_idx,:]\n",
    "        \n",
    "        _, cost_val = sess.run([train,cost],feed_dict={X:batch_x,Y:batch_y,keep_prob:1.0})\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(cost_val)\n",
    "\n",
    "#Accuracy (정확도 측정)\n",
    "df_x_test = test_df[[\"Pclass\",\"Sex\",\"Age\",\"Parch\",\"Fare\"]]\n",
    "df_y_test = test_df[[\"Survived\"]]\n",
    "\n",
    "#Accuracy 정확도 측정\n",
    "predict = tf.argmax(H,1) #열방향으로 큰값 추출\n",
    "correct = tf.equal(predict, tf.argmax(Y,1)) #실제값과 예측값 비교\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,dtype=tf.float32))\n",
    "\n",
    "x_data_t = MinMaxScaler().fit_transform(df_x_test.values)\n",
    "y_data_t = df_y_test \n",
    "\n",
    "result = sess.run(accuracy,feed_dict={X:x_data_t,\n",
    "                                     Y:y_data_t,keep_prob:1})\n",
    "\n",
    "print(\"정확도:{}\".format(result))\n",
    "\n",
    "#===============================================================\n",
    "#predict check\n",
    "df_t = pd.read_csv(\"./data/titanic/test.csv\",sep=\",\",skiprows=0)\n",
    "\n",
    "df_t2 = df_t[[\"PassengerId\",\"Pclass\",\"Sex\",\"Age\",\"Parch\",\"Fare\"]]\n",
    "#df_t2.dropna(how=\"any\", inplace=True)\n",
    "df_t2[\"Sex\"] = df_t2['Sex'].apply(lambda x: '1' if x == 'male' else '2')\n",
    "\n",
    "df_ids = df_t2[[\"PassengerId\"]]\n",
    "df_test = df_t2[[\"Pclass\",\"Sex\",\"Age\",\"Parch\",\"Fare\"]]\n",
    "#print(df_t2)\n",
    "test_x_data = MinMaxScaler().fit_transform(df_test.values)\n",
    "#print(test_x_data)\n",
    "\n",
    "#predict = tf.argmax(H,1)\n",
    "predict = tf.cast(H > 0.5, dtype=tf.float32) #예측값\n",
    "result = sess.run(predict, feed_dict={X:test_x_data, keep_prob: 1.0})\n",
    "#print(type(result))\n",
    "print(result)\n",
    "df = pd.DataFrame({\n",
    "    'PassengerId': [str(i[0]) for i in df_ids.values],\n",
    "    'Survived': [int(i[0]) for i in result]\n",
    "})\n",
    "print(df)\n",
    "\n",
    "df.to_csv('./data/titanic/gender_submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpu_env] *",
   "language": "python",
   "name": "conda-env-cpu_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
