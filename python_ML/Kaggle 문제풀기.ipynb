{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(29400, 784)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(29400, 10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20250611\n",
      "0.004919688\n",
      "0.0032596055\n",
      "0.0011879979\n",
      "1.9788858e-05\n",
      "5.275711e-06\n",
      "1.0633271e-06\n",
      "2.0503909e-07\n",
      "3.6954845e-08\n",
      "7.152556e-09\n",
      "정확도 :[1.0]\n"
     ]
    }
   ],
   "source": [
    "## Kaggle 수행평가\n",
    "## Digit Recognizer 문제풀기\n",
    "# train.csv => 7:3 비율로 나누어서 학습 및 accuracy 확인\n",
    "# batch 형태로 데이터를 읽어들여서 학습 \n",
    "# test.csv를 이용하여 prediction결과 도출\n",
    "# 해당 결과를 이용하여 submission.csv 파일을 생성한 후 Kaggle site에 제출 및 확인. pd.to csv 파일 만드는거 있음\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "# tensotflow 그래프 초기화\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Data Loading\n",
    "df = pd.read_csv(\"./data/Digit Recognizer/train.csv\", sep = \",\" ,skiprows=0)\n",
    "# test = pd.read_csv(\"./data/Digit Recognizer/test.csv\", sep = \",\")\n",
    "\n",
    "# display(df.shape)\n",
    "# display(df.head())\n",
    "\n",
    "# train 7:3으로 나누기\n",
    "step_cnt = int(df.shape[0] * 0.7)\n",
    "# print(step_cnt) # 29399\n",
    "train_df = df.loc[:step_cnt,:]\n",
    "test_df  = df.loc[step_cnt:,:]\n",
    "# display(train_df)\n",
    "\n",
    "#  x데이터를 0과 1사이의 값으로 변경\n",
    "\n",
    "#training data set - train\n",
    "x_data = MinMaxScaler().fit_transform(train_df.loc[:,\"pixel0\":])\n",
    "print(type(x_data))\n",
    "y_data = tf.one_hot(train_df[\"label\"],10)\n",
    "sess = tf.Session()\n",
    "y_data = sess.run(y_data)\n",
    "display(y_data)\n",
    "\n",
    "#training data set - test\n",
    "x_data_test = MinMaxScaler().fit_transform(test_df.loc[:,\"pixel0\":])\n",
    "y_data_test = tf.one_hot(test_df[\"label\"],10)\n",
    "y_data_test = sess.run(y_data_test)\n",
    "\n",
    "display(x_data.shape) # (29400, 784)\n",
    "display(y_data.shape) # (29400, 10)\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None, 784], dtype=tf.float32) \n",
    "Y = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "# W1 = tf.Variable(tf.random_normal([784,256]), name=\"weight1\")\n",
    "W1 = tf.get_variable(\"weight1\", shape=[784,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]), name=\"bias1\")\n",
    "# layer1 = tf.nn.relu(tf.matmul(X,W1) + b1) \n",
    "_layer1 = tf.nn.relu(tf.matmul(X,W1) + b1) \n",
    "layer1 = tf.nn.dropout(_layer1, keep_prob = keep_prob)\n",
    "\n",
    "# W2 = tf.Variable(tf.random_normal([256,256]), name=\"weight2\")\n",
    "W2 =  tf.get_variable(\"weight2\", shape=[256,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]), name=\"bias2\")\n",
    "# layer2 = tf.nn.relu(tf.matmul(layer1,W2) + b2)\n",
    "_layer2 = tf.nn.relu(tf.matmul(layer1,W2) + b2)\n",
    "layer2 = tf.nn.dropout(_layer2, keep_prob = keep_prob)\n",
    "\n",
    "# W3 = tf.Variable(tf.random_normal([256,10]), name=\"weight3\")\n",
    "W3 = tf.get_variable(\"weight3\", shape=[256,10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]), name=\"bias3\")\n",
    "\n",
    "# Hypothesis\n",
    "logits = tf.matmul(layer2,W3) + b3  \n",
    "H = tf.nn.relu(logits)\n",
    "\n",
    "# Cost Function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "\n",
    "# train node 생성\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "# Session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# epoch 학습진행 , 데이터 슬라이싱\n",
    "training_epoch = 100\n",
    "batch_size = 100 \n",
    "\n",
    "for step in range(training_epoch): \n",
    "    num_of_iter = int( x_data.shape[0] /  batch_size) \n",
    "    cost_val = 0\n",
    "    # (x데이터,y데이터)\n",
    "    start_idx = 0\n",
    "    end_idx = 0\n",
    "    for i in range(num_of_iter):\n",
    "        # batch_x, batch_y = mnist.train.next_batch(batch_size) \n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        \n",
    "        batch_x = x_data[start_idx:end_idx,:]\n",
    "        batch_y = y_data[start_idx:end_idx,:]\n",
    "        \n",
    "        _, cost_val = sess.run([train,cost], feed_dict={X:batch_x , Y:batch_y, keep_prob:1.0})\n",
    "        \n",
    "    if step % 10 == 0:\n",
    "        print(cost_val)     \n",
    "\n",
    "# Accuracy(정확도) 측정\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "result = sess.run([accuracy], feed_dict={X:x_data, Y:y_data, keep_prob:1.0})\n",
    "\n",
    "print(\"정확도 :{}\".format(result))\n",
    "\n",
    "#predict check\n",
    "df_test = pd.read_csv(\"./data/Digit Recognizer/test.csv\",sep=\",\",skiprows=0)\n",
    "x_data_test = MinMaxScaler().fit_transform(df_test)\n",
    "\n",
    "predict = tf.argmax(H,1)\n",
    "result = sess.run(predict, feed_dict={X:x_data_test, keep_prob: 1.0})\n",
    "df = pd.DataFrame({\n",
    "    \"ImageId\": [i for i in range(1,28001)],\n",
    "    \"Label\": result\n",
    "})\n",
    "#display(result)\n",
    "df.to_csv(\"./data/Digit Recognizer/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(29400, 784)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(29400, 10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 7, 7, 64)\n",
      "0.09422663\n",
      "0.0014924891\n",
      "0.0020208089\n",
      "1.3721731e-05\n",
      "3.683524e-07\n",
      "3.814692e-08\n",
      "3.5762782e-09\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "정확도 :[1.0]\n"
     ]
    }
   ],
   "source": [
    "## Digit Recognizer 문제풀기 + CNN적용\n",
    "# train.csv => 7:3 비율로 나누어서 학습 및 accuracy 확인\n",
    "# batch 형태로 데이터를 읽어들여서 학습 \n",
    "# test.csv를 이용하여 prediction결과 도출\n",
    "# 해당 결과를 이용하여 submission.csv 파일을 생성한 후 Kaggle site에 제출 및 확인. pd.to csv 파일 만드는거 있음\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "# tensotflow 그래프 초기화\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Data Loading\n",
    "df = pd.read_csv(\"./data/Digit Recognizer/train.csv\", sep = \",\" ,skiprows=0)\n",
    "# test = pd.read_csv(\"./data/Digit Recognizer/test.csv\", sep = \",\")\n",
    "\n",
    "# display(df.shape)\n",
    "# display(df.head())\n",
    "\n",
    "# train 7:3으로 나누기\n",
    "step_cnt = int(df.shape[0] * 0.7)\n",
    "# print(step_cnt) # 29399\n",
    "train_df = df.loc[:step_cnt,:]\n",
    "test_df  = df.loc[step_cnt:,:]\n",
    "# display(train_df)\n",
    "\n",
    "#  x데이터를 0과 1사이의 값으로 변경\n",
    "\n",
    "#training data set - train\n",
    "x_data = MinMaxScaler().fit_transform(train_df.loc[:,\"pixel0\":])\n",
    "print(type(x_data))\n",
    "y_data = tf.one_hot(train_df[\"label\"],10)\n",
    "sess = tf.Session()\n",
    "y_data = sess.run(y_data)\n",
    "display(y_data)\n",
    "\n",
    "#training data set - test\n",
    "x_data_test = MinMaxScaler().fit_transform(test_df.loc[:,\"pixel0\":])\n",
    "y_data_test = tf.one_hot(test_df[\"label\"],10)\n",
    "y_data_test = sess.run(y_data_test)\n",
    "\n",
    "display(x_data.shape) # (29400, 784)\n",
    "display(y_data.shape) # (29400, 10)\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None, 784], dtype=tf.float32) \n",
    "Y = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "# Convolution \n",
    "X_img = tf.reshape(X,[-1,28,28,1])\n",
    "\n",
    "## Convolution Layer1 \n",
    "L1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3,3], padding=\"SAME\", strides=1, activation=tf.nn.relu) # 3x3 이미지 32개 # kernel_size : 필터의 크기\n",
    "L1 = tf.layers.max_pooling2d(inputs=L1, pool_size=[2,2], padding=\"SAME\", strides=2)\n",
    "\n",
    "## Convolution Layer2\n",
    "L2 = tf.layers.conv2d(inputs=L1, filters=64, kernel_size=[3,3], padding=\"SAME\", strides=1, activation=tf.nn.relu) # 3x3 이미지 64개 # kernel_size : 필터의 크기\n",
    "L2 = tf.layers.max_pooling2d(inputs=L2, pool_size=[2,2], padding=\"SAME\", strides=2)\n",
    "# karas.layers.max_pooling2d \n",
    "print(L2.shape) \n",
    "\n",
    "L2 = tf.reshape(L2,[-1,7*7*64])\n",
    "\n",
    "# Weight & bias\n",
    "# W1 = tf.Variable(tf.random_normal([784,256]), name=\"weight1\")\n",
    "W1 = tf.get_variable(\"weight1\", shape=[7*7*64,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]), name=\"bias1\")\n",
    "# layer1 = tf.nn.relu(tf.matmul(X,W1) + b1) \n",
    "_layer1 = tf.nn.relu(tf.matmul(L2,W1) +b1)\n",
    "layer1 = tf.nn.dropout(_layer1, keep_prob = keep_prob)\n",
    "\n",
    "# W2 = tf.Variable(tf.random_normal([256,256]), name=\"weight2\")\n",
    "W2 =  tf.get_variable(\"weight2\", shape=[256,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]), name=\"bias2\")\n",
    "# layer2 = tf.nn.relu(tf.matmul(layer1,W2) + b2)\n",
    "_layer2 = tf.nn.relu(tf.matmul(layer1,W2) + b2)\n",
    "layer2 = tf.nn.dropout(_layer2, keep_prob = keep_prob)\n",
    "\n",
    "# W3 = tf.Variable(tf.random_normal([256,10]), name=\"weight3\")\n",
    "W3 = tf.get_variable(\"weight3\", shape=[256,10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]), name=\"bias3\")\n",
    "\n",
    "# Hypothesis\n",
    "logits = tf.matmul(layer2,W3) + b3  \n",
    "H = tf.nn.relu(logits)\n",
    "\n",
    "# Cost Function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "\n",
    "# train node 생성\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "# trian = optimizer.minimize(cost)\n",
    "\n",
    "# Session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# epoch 학습진행 , 데이터 슬라이싱\n",
    "training_epoch = 100\n",
    "batch_size = 100 \n",
    "\n",
    "for step in range(training_epoch): \n",
    "    num_of_iter = int( x_data.shape[0] /  batch_size) \n",
    "    cost_val = 0\n",
    "    # (x데이터,y데이터)\n",
    "    start_idx = 0\n",
    "    end_idx = 0\n",
    "    for i in range(num_of_iter):\n",
    "        # batch_x, batch_y = mnist.train.next_batch(batch_size) \n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "#         if end_idx > :\n",
    "#             end_idx = batch_size\n",
    "        \n",
    "        batch_x = x_data[start_idx:end_idx,:]\n",
    "        batch_y = y_data[start_idx:end_idx,:]\n",
    "        \n",
    "        _, cost_val = sess.run([train,cost], feed_dict={X:batch_x , Y:batch_y, keep_prob:1.0}) # 0.5-0.7로\n",
    "        \n",
    "    if step % 10 == 0:\n",
    "        print(cost_val)     \n",
    "\n",
    "# Accuracy(정확도) 측정\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "result = sess.run([accuracy], feed_dict={X:x_data, Y:y_data, keep_prob:1.0})\n",
    "\n",
    "print(\"정확도 :{}\".format(result))\n",
    "\n",
    "#predict check\n",
    "df_test = pd.read_csv(\"./data/Digit Recognizer/test.csv\",sep=\",\",skiprows=0)\n",
    "x_data_test = MinMaxScaler().fit_transform(df_test)\n",
    "\n",
    "predict = tf.argmax(H,1)\n",
    "result = sess.run(predict, feed_dict={X:x_data_test, keep_prob: 1.0})\n",
    "df = pd.DataFrame({\n",
    "    \"ImageId\": [i for i in range(1,28001)],\n",
    "    \"Label\": result\n",
    "})\n",
    "#display(result)\n",
    "df.to_csv(\"./data/Digit Recognizer/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(398, 5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(398, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data_test shape: (317, 5)\n",
      "y_data_test shape: (317, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "0.6903784\n",
      "0.6799202\n",
      "0.6401634\n",
      "0.6062219\n",
      "0.56409854\n",
      "0.52428293\n",
      "0.4993717\n",
      "0.48544303\n",
      "0.47652692\n",
      "0.47096008\n",
      "정확도 :[1.0]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "## Titanic: Machine Learning from Disaster 문제 풀기\n",
    "# train.csv => 7:3 비율로 나누어서 학습 및 accuracy 확인\n",
    "# batch 형태로 데이터를 읽어들여서 학습 \n",
    "# test.csv를 이용하여 prediction결과 도출\n",
    "# 해당 결과를 이용하여 gender_submission.csv 파일을 생성한 후 Kaggle site에 제출 및 확인. pd.to csv 파일 만드는거 있음\n",
    "\n",
    "#PassengerId 탑승자id,Survived 구조여부 (1 구조 0 구조못함)\n",
    "#Pclass 좌석등급, Name 이름, Sex 성별, Age 나이,SibSp 형제수,Parch 부모/자식수,\n",
    "#Ticket 티켓번호,Fare 요금, Cabin 객실, Embarked 탑승한 곳\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "# tensotflow 그래프 초기화\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Data Loading\n",
    "df = pd.read_csv(\"./data/Titanic_/train.csv\", sep=\",\", skiprows=0)\n",
    "# display(df)\n",
    "\n",
    "# 데이터 정제\n",
    "df2 = df[[\"Survived\",\"Pclass\",\"Sex\",\"Age\",\"Parch\",\"Fare\"]]\n",
    "df2.dropna(how=\"any\", inplace=True)\n",
    "\n",
    "df2[\"Sex\"] = df2['Sex'].apply(lambda x: '1' if x == 'male' else '2')\n",
    "\n",
    "# 데이터 정제\n",
    "# df.drop([\"Name\",\"Ticket\",\"Cabin\"], axis = 1, inplace = True )\n",
    "# df.dropna(how = \"any\", inplace = True) # NaN이 있는 모든 row 삭제\n",
    "# df.loc[df[\"Sex\"] == \"male\",\"Sex\"] = 1 # 남자는 1 여자는 2\n",
    "# df.loc[df[\"Sex\"] == \"female\",\"Sex\"] = 2\n",
    "# df.loc[:,\"Age\"] = (df[\"Age\"]//10) # 연령을 처리하기 쉽게 하기위해 나누기10 / 0-9세는 0, 10-19세는 1, 20-29는 2 ... 표현\n",
    "# df.loc[df[\"Embarked\"] == \"S\",\"Embarked\"] = 1  # Embarked == 1: Southampton\n",
    "# df.loc[df[\"Embarked\"] == \"C\",\"Embarked\"] = 2  # Embarked == 2: Cherbourg\n",
    "# df.loc[df[\"Embarked\"] == \"Q\",\"Embarked\"] = 3  # Embarked == 3: Queenstown\n",
    "# df[\"Fare\"] = pd.qcut(df[\"Fare\"],5,labels=[1,2,3,4,5]) # 전체 요금을 5개로 나눠줌\n",
    "# display(df)\n",
    "\n",
    "# 7:3으로 나누기\n",
    "step_cnt = int(df2.shape[0] * 0.7)\n",
    "# print(step_cnt) # 623\n",
    "train_df = df2.loc[:step_cnt,:]\n",
    "test_df  = df2.loc[step_cnt:,:]\n",
    "# bdisplay(train_df)\n",
    "\n",
    "# train\n",
    "df_x = train_df[[\"Pclass\",\"Sex\",\"Age\",\"Parch\",\"Fare\"]]\n",
    "df_y = train_df[[\"Survived\"]]\n",
    "print(type(df_x))\n",
    "# display(df_x)\n",
    "\n",
    "# test\n",
    "df_x_test = test_df[[\"Pclass\",\"Sex\",\"Age\",\"Parch\",\"Fare\"]]\n",
    "df_y_test = test_df[[\"Survived\"]]\n",
    "\n",
    "#training data set - train\n",
    "x_data = MinMaxScaler().fit_transform(df_x)\n",
    "y_data = df_y.values\n",
    "\n",
    "display(x_data.shape) # (398, 5)\n",
    "display(y_data.shape) # (398, 1)\n",
    "\n",
    "#training data set - test\n",
    "x_data_test = MinMaxScaler().fit_transform(df_x_test.values)\n",
    "y_data_test = df_y_test\n",
    "\n",
    "print(\"x_data_test shape: {}\".format(x_data_test.shape)) # (317, 5)\n",
    "print(\"y_data_test shape: {}\".format(y_data_test.shape)) # (317, 1)\n",
    "print(type(x_data_test))\n",
    "print(type(y_data_test))\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None, 5], dtype=tf.float32) \n",
    "Y = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W1 = tf.get_variable(\"weight1\", shape=[5,20], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([20]), name=\"bias1\")\n",
    "# layer1 = tf.nn.relu(tf.matmul(X,W1) + b1) \n",
    "_layer1 = tf.nn.relu(tf.matmul(X,W1) + b1) \n",
    "layer1 = tf.nn.dropout(_layer1, keep_prob = keep_prob)\n",
    "\n",
    "W2 =  tf.get_variable(\"weight2\", shape=[20,20], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([20]), name=\"bias2\")\n",
    "# layer2 = tf.nn.relu(tf.matmul(layer1,W2) + b2)\n",
    "_layer2 = tf.nn.relu(tf.matmul(layer1,W2) + b2)\n",
    "layer2 = tf.nn.dropout(_layer2, keep_prob = keep_prob)\n",
    "\n",
    "\n",
    "W3 = tf.get_variable(\"weight3\", shape=[20,1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([1]), name=\"bias3\")\n",
    "\n",
    "# Hypothesis\n",
    "logits = tf.matmul(layer2,W3) + b3  \n",
    "H = tf.nn.relu(logits)\n",
    "\n",
    "# Cost Function\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "\n",
    "# train node 생성\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "# Session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# epoch 학습진행 , 데이터 슬라이싱\n",
    "training_epoch = 100\n",
    "batch_size = 100 \n",
    "\n",
    "for step in range(training_epoch): \n",
    "    num_of_iter = int( x_data.shape[0] /  batch_size) \n",
    "    cost_val = 0\n",
    "    # (x데이터,y데이터)\n",
    "    start_idx = 0\n",
    "    end_idx = 0\n",
    "    for i in range(num_of_iter):\n",
    "        # batch_x, batch_y = mnist.train.next_batch(batch_size) \n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        \n",
    "        batch_x = x_data[start_idx:end_idx,:]\n",
    "        batch_y = y_data[start_idx:end_idx,:]\n",
    "        \n",
    "        _, cost_val = sess.run([train,cost], feed_dict={X:batch_x , Y:batch_y, keep_prob:1.0})\n",
    "        \n",
    "    if step % 10 == 0:\n",
    "        print(cost_val)     \n",
    "\n",
    "\n",
    "# Accuracy 정확도\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "# print(type(df_x_test))\n",
    "\n",
    "result = sess.run([accuracy], feed_dict={X:x_data_test, Y:y_data_test, keep_prob:1.0})\n",
    "\n",
    "print(\"정확도 :{}\".format(result))\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "#predict check\n",
    "df_t = pd.read_csv(\"./data/Titanic_/test.csv\",sep=\",\",skiprows=0)\n",
    "\n",
    "df_t2 = df_t[[\"PassengerId\",\"Pclass\",\"Sex\",\"Age\",\"Parch\",\"Fare\"]]\n",
    "\n",
    "df_t2[\"Sex\"] = df_t2['Sex'].apply(lambda x: '1' if x == 'male' else '2')\n",
    "\n",
    "df_ids = df_t2[[\"PassengerId\"]]\n",
    "df_test = df_t2[[\"Pclass\",\"Sex\",\"Age\",\"Parch\",\"Fare\"]]\n",
    "#print(df_t2)\n",
    "test_x_data = MinMaxScaler().fit_transform(df_test.values)\n",
    "#print(test_x_data)\n",
    "\n",
    "# predict = tf.argmax(H,1)\n",
    "predict = tf.cast(H >= 0.5, dtype=tf.float32)\n",
    "result = sess.run(predict, feed_dict={X:test_x_data, keep_prob: 1.0})\n",
    "print(result)\n",
    "df = pd.DataFrame({\n",
    "    \"PassengerId\": [ str(i[0]) for i in df_ids.values ],\n",
    "    \"Survived\": [ int(i[0]) for i in result ]\n",
    "})\n",
    "# print(df)\n",
    "#display(result)\n",
    "df.to_csv(\"./data/Titanic_/gender_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpu_env] *",
   "language": "python",
   "name": "conda-env-cpu_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
