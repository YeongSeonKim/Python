{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2019-07-10\n",
    "## 1.tensorflow module 설치 ( cpu용 )\n",
    "## > conda install tensorflow\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "## 2. Hello World 출력\n",
    "## 상수를 하나 만들어요( 상수 Node 생성 )\n",
    "## Tensorflow Node는 숫자 연산과 데이터 입출력을 담당.\n",
    "## Session을 이용해서 Node를 실행시켜야지 Node가 가지고 있는 데이터를 출력할 수 있다.\n",
    "\n",
    "my_node = tf.constant(\"Hello World\")\n",
    "\n",
    "sess = tf.Session()\n",
    "print(sess.run(my_node).decode()) # b'Hello World'에서 b : binary text / .decode()는 원래 형태대로 출력해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TensorFlow : Google이 만든 machine library \n",
    "##              open source library\n",
    "##              수학적 계산을 하기 위한 library\n",
    "##              data flow graph를 이용해요~! ( node와 edge로 이루어진 방향성있는)\n",
    "\n",
    "## data flow graph는 Node와 Edge로 구성된 벙향성 있는 graph\n",
    "\n",
    "## Node : 데이터의 입출력과 수학적 연산\n",
    "## Edge : Tensor를 Node로 실어 나르는 역할\n",
    "## Tensor : 동적 크기의 다차원 배열을 지칭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.0, 20.0, 30.0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "node1 = tf.constant(10, dtype=tf.float32)\n",
    "node2 = tf.constant(20, dtype=tf.float32)\n",
    "\n",
    "node3 = node1 + node2\n",
    "\n",
    "## 그래프를 실행시키기 위해 runner의 역할을 하는 Session객체가 있어야한다.\n",
    "sess = tf.Session()\n",
    "# print(sess.run(node3))\n",
    "# print(sess.run(node1,node2)) # 복수개의 노드를 실행하려면 배열형태로 입력해야함!!\n",
    "print(sess.run([node1,node2,node3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#  placeholder 를 이용\n",
    "# 2개의 수를 입력으로 받아서 더하는 프로그램\n",
    "# placeholder: 입력 데이터를 받아들이기위한 저장공간  / 값이 없을때 사용\n",
    "#               데이터를 담는 박스같은 개념,\n",
    "node1 = tf.placeholder(dtype=tf.float32) \n",
    "node2 = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "node3 = node1 + node2\n",
    "\n",
    "sess = tf.Session()\n",
    "result = sess.run(node3, feed_dict={node1 : 10 , node2 : 20}) # key와 vlaue의 값으로 할당\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_27:0\", shape=(3,), dtype=int32)\n",
      "Tensor(\"Cast_2:0\", shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "node1 = tf.constant([10,20,30], dtype=tf.int32)\n",
    "print(node1)\n",
    "node2 = tf.cast(node1, dtype=tf.float32)\n",
    "print(node2)\n",
    "\n",
    "# ★연산할때마다 항상 데이터 타입 같게!!!★"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W:[-0.16185987],b:[-0.6909872],cost:12.635558128356934\n",
      "W:[1.0322062],b:[-0.07321213],cost:0.000772638653870672\n",
      "W:[1.0156443],b:[-0.03556334],cost:0.0001823117636376992\n",
      "W:[1.0075994],b:[-0.01727502],cost:4.301795343053527e-05\n",
      "W:[1.0036913],b:[-0.00839141],cost:1.0150321941182483e-05\n",
      "W:[1.0017934],b:[-0.0040767],cost:2.395647015873692e-06\n",
      "W:[1.0008715],b:[-0.00198126],cost:5.658911845785042e-07\n",
      "W:[1.0004239],b:[-0.00096373],cost:1.338753463642206e-07\n",
      "W:[1.0002068],b:[-0.00046945],cost:3.176247886926831e-08\n",
      "W:[1.0001013],b:[-0.00022938],cost:7.597413365090233e-09\n",
      "W:[1.0000501],b:[-0.00011297],cost:1.8478895080775715e-09\n",
      "W:[1.0000237],b:[-5.3879685e-05],cost:4.154543375989306e-10\n",
      "W:[1.0000125],b:[-2.7774835e-05],cost:1.1257483834015147e-10\n",
      "W:[1.0000076],b:[-1.6562792e-05],cost:4.002724193763463e-11\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# training data set\n",
    "x = [1,2,3]\n",
    "y = [1,2,3] # label\n",
    "\n",
    "# 선형회귀 ( linear regression )\n",
    "# 가장 큰 목표는 가설의 완성\n",
    "# 가설( hypothesis ) = Wx + b\n",
    "# W와 b를 정의( 변수로 )\n",
    "# Weight 와 bias의 정의\n",
    "W = tf.Variable(tf.random_normal([1]),name=\"weight\") # tensorflow node로 만들꺼임 # Variable : 변수처럼 동작하는 데이터\n",
    "# random_nomal() :  표준정규분포  , 배열형태로 어떻게 출력할지 적어줘야함\n",
    "b = tf.Variable(tf.random_normal([1]),name=\"bias\")\n",
    "\n",
    "# W도  node b도 node => 결과 값도 node\n",
    "\n",
    "# Hypothesis( 가설 )\n",
    "# 우리의 최종 목적은 training data에 가장 근접한 Hypothesis로 만드는 것 (W와 b를 결정)\n",
    "# 잘만들어진 가설은 W가 1에 가깝고 b가 0에 가까워야한다.\n",
    "H = W * x + b # H(x) = Wx + b 를 수학적 식으로 표현\n",
    "\n",
    "# cost(loss) Function\n",
    "# 우리의 목적은 cost 함수를 최소로 만드는 W와 b를 구하는 거다.\n",
    "cost = tf.reduce_mean(tf.square(H - y)) # reduce_mean() : 평균구하는거 / tf.square() : 제곱하라는거 \n",
    "\n",
    "## cost function minimize (최소화)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01) #  tf.train.GradientDescentOptimizer() - 미분\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "## runner 생성 - 그래프를 생성하기위한 session 객체\n",
    "sess = tf.Session()\n",
    "## global variable의 초기화\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "## 학습을 진행  - 한번에 최소값을 얻어낼수 없고 기본값보다 작은값을 얻을 수 있도록 하는 식을 현재 작성한거임 \n",
    "## => 반복하다보면 제일작은 값이나옴\n",
    "for step in range(4000):\n",
    "    # 반복하면서 train을 실행시킬꺼임\n",
    "    _, w_val, b_val, cost_val =  sess.run([train,W,b,cost])\n",
    "    if step % 300 == 0: \n",
    "        print(\"W:{},b:{},cost:{}\".format(w_val,b_val,cost_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.05527\n",
      "0.021459691\n",
      "0.0035507125\n",
      "0.0005874945\n",
      "9.720442e-05\n",
      "1.6083595e-05\n",
      "2.6623516e-06\n",
      "4.413751e-07\n",
      "7.353566e-08\n",
      "1.2263172e-08\n",
      "2.0439188e-09\n",
      "4.129106e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n",
      "1.5285195e-10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([900.9967], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2019-07-11\n",
    "import tensorflow as tf\n",
    "\n",
    "## training data set\n",
    "x = tf.placeholder(dtype=tf.float32) # placeholderce() : 입력 파라매터를 받는 변수\n",
    "y = tf.placeholder(dtype=tf.float32) \n",
    "\n",
    "x_data = [1,2,3,4]\n",
    "y_data = [4,7,10,13] \n",
    "\n",
    "# Weight & bias : 초기화를 해줘야함\n",
    "W = tf.Variable(tf.random_normal([1]), name=\"weight\")  # 초기값을 줄때 0과 근_?한 랜덤하게 줄꺼임 / ([]) 배열형태로 스칼라형태로 쓴다./ name : 변수마다 이름부여\n",
    "b  = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# x,y,W,b => tensorflow node , H도 결과적으로 tensorflow node\n",
    "\n",
    "# Hypothesis : 가설\n",
    "H = W * x + b  # 1차식에 대한 수학식\n",
    "# Hypothesis를 정의할려면 weight 와 bias가 정의가 먼저 되어야함\n",
    "\n",
    "# cost(loss) Function\n",
    "cost = tf.reduce_mean(tf.square(H - y)) # squre() : 제곱을 의미\n",
    "\n",
    "# cost function을 최소화 시키기 위한 작업\n",
    "# GRADIENT DESCENT ALGORITHM 사용 : optimizer \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# tensorflow 그래프를 실행시키기 위한 Session & 초기화\n",
    "## runner 생성 - 그래프를 생성하기위한 session 객체\n",
    "sess = tf.Session()\n",
    "## global variable의 초기화\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습 : 반복적으로 실행시켜서 w와 b의 값을 구해내는것\n",
    "for step in range(300000): \n",
    "    # range(?) : ?의 반복횟수 값을 크게 하면 좋은데 높게 잡아버리면 시간이 오래걸린다...\n",
    "    _, cost_val = sess.run([train,cost], feed_dict={x:x_data, y:y_data}) \n",
    "    # 이렇게하면 w와 b가 제대로 나오는지는 확인할 수 없다. 그래서 cost을 같이 뽑아냅 \n",
    "    # cost값을 알면 제대로 학습이 된지 알수 있다. / _. : 사용하지 않겠다는 의미\n",
    "    if step % 300 == 0:\n",
    "        print(cost_val)\n",
    "\n",
    "# prediction : 예측값 추출 \n",
    "sess.run(H, feed_dict={x:[300]}) # x값이 주어졌을때 y값 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd   # pandas로 파일읽어서 데이터 읽는 작업을 하는게 좋음\n",
    "import warnings       # action=ignore\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ozone</th>\n",
       "      <th>Solar.R</th>\n",
       "      <th>Wind</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>67</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>72</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>12.6</td>\n",
       "      <td>74</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>62</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.3</td>\n",
       "      <td>56</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.9</td>\n",
       "      <td>66</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23.0</td>\n",
       "      <td>299.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>65</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>59</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>194.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>69</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.9</td>\n",
       "      <td>74</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>69</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>66</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>68</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>34.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>66</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>57</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>30.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>68</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>62</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>59</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>11.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>73</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>32.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>57</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>266.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>57</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>23.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>67</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>45.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>81</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>115.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>79</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>96.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>91</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>78.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>92</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>73.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>93</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>91.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>93</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>47.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>87</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>32.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>84</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>20.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>80</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>23.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>78</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>21.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>75</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>24.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>73</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>44.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>81</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>21.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>28.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>77</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>9.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>10.9</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>13.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>46.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>78</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>18.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>13.8</td>\n",
       "      <td>67</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>13.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>24.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>68</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>16.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>82</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>13.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>12.6</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>23.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>36.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>81</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>7.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>69</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>63</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>30.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>6.9</td>\n",
       "      <td>70</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>NaN</td>\n",
       "      <td>145.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>77</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>14.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>75</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>18.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>20.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>68</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Ozone  Solar.R  Wind  Temp  Month  Day\n",
       "0     41.0    190.0   7.4    67      5    1\n",
       "1     36.0    118.0   8.0    72      5    2\n",
       "2     12.0    149.0  12.6    74      5    3\n",
       "3     18.0    313.0  11.5    62      5    4\n",
       "4      NaN      NaN  14.3    56      5    5\n",
       "5     28.0      NaN  14.9    66      5    6\n",
       "6     23.0    299.0   8.6    65      5    7\n",
       "7     19.0     99.0  13.8    59      5    8\n",
       "8      8.0     19.0  20.1    61      5    9\n",
       "9      NaN    194.0   8.6    69      5   10\n",
       "10     7.0      NaN   6.9    74      5   11\n",
       "11    16.0    256.0   9.7    69      5   12\n",
       "12    11.0    290.0   9.2    66      5   13\n",
       "13    14.0    274.0  10.9    68      5   14\n",
       "14    18.0     65.0  13.2    58      5   15\n",
       "15    14.0    334.0  11.5    64      5   16\n",
       "16    34.0    307.0  12.0    66      5   17\n",
       "17     6.0     78.0  18.4    57      5   18\n",
       "18    30.0    322.0  11.5    68      5   19\n",
       "19    11.0     44.0   9.7    62      5   20\n",
       "20     1.0      8.0   9.7    59      5   21\n",
       "21    11.0    320.0  16.6    73      5   22\n",
       "22     4.0     25.0   9.7    61      5   23\n",
       "23    32.0     92.0  12.0    61      5   24\n",
       "24     NaN     66.0  16.6    57      5   25\n",
       "25     NaN    266.0  14.9    58      5   26\n",
       "26     NaN      NaN   8.0    57      5   27\n",
       "27    23.0     13.0  12.0    67      5   28\n",
       "28    45.0    252.0  14.9    81      5   29\n",
       "29   115.0    223.0   5.7    79      5   30\n",
       "..     ...      ...   ...   ...    ...  ...\n",
       "123   96.0    167.0   6.9    91      9    1\n",
       "124   78.0    197.0   5.1    92      9    2\n",
       "125   73.0    183.0   2.8    93      9    3\n",
       "126   91.0    189.0   4.6    93      9    4\n",
       "127   47.0     95.0   7.4    87      9    5\n",
       "128   32.0     92.0  15.5    84      9    6\n",
       "129   20.0    252.0  10.9    80      9    7\n",
       "130   23.0    220.0  10.3    78      9    8\n",
       "131   21.0    230.0  10.9    75      9    9\n",
       "132   24.0    259.0   9.7    73      9   10\n",
       "133   44.0    236.0  14.9    81      9   11\n",
       "134   21.0    259.0  15.5    76      9   12\n",
       "135   28.0    238.0   6.3    77      9   13\n",
       "136    9.0     24.0  10.9    71      9   14\n",
       "137   13.0    112.0  11.5    71      9   15\n",
       "138   46.0    237.0   6.9    78      9   16\n",
       "139   18.0    224.0  13.8    67      9   17\n",
       "140   13.0     27.0  10.3    76      9   18\n",
       "141   24.0    238.0  10.3    68      9   19\n",
       "142   16.0    201.0   8.0    82      9   20\n",
       "143   13.0    238.0  12.6    64      9   21\n",
       "144   23.0     14.0   9.2    71      9   22\n",
       "145   36.0    139.0  10.3    81      9   23\n",
       "146    7.0     49.0  10.3    69      9   24\n",
       "147   14.0     20.0  16.6    63      9   25\n",
       "148   30.0    193.0   6.9    70      9   26\n",
       "149    NaN    145.0  13.2    77      9   27\n",
       "150   14.0    191.0  14.3    75      9   28\n",
       "151   18.0    131.0   8.0    76      9   29\n",
       "152   20.0    223.0  11.5    68      9   30\n",
       "\n",
       "[153 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 1. pandas를 이용 데이터 불러와서 데이터 프레임만들기\n",
    "df = pd.read_csv(\"./data/ozone/ozone.csv\", sep=\",\") # sep :  구분자\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153, 2)\n",
      "(116, 2)\n"
     ]
    }
   ],
   "source": [
    "## 온도에 따른 오존양 예측 / 오존양에 미치는 요소들 :Solar.R, Wind, Temp\n",
    "## 필요한 columns 먼저 추출\n",
    "\n",
    "# Fancy indexing\n",
    "df2 = df[[\"Ozone\",\"Temp\"]] \n",
    "# 결측값을 처리(제거)\n",
    "df3 = df2.dropna(how=\"any\", inplace=False)  \n",
    "print(df2.shape)\n",
    "print(df3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 이렇게 준비한 데이터가 linear한 데이터인지 확인\n",
    "# plt.scatter() : 데이터가 흩어진 형태를 보여줌 (어떻게 분포되어있는지)\n",
    "plt.scatter(df3[\"Temp\"],df3[\"Ozone\"]) # X 데이터: Temp, Y 데이터 : Ozone\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost : 0.33531370759010315\n",
      "cost : 0.019864169880747795\n",
      "cost : 0.019817078486084938\n",
      "cost : 0.01981682889163494\n",
      "cost : 0.019816827028989792\n",
      "cost : 0.019816827028989792\n",
      "cost : 0.019816827028989792\n",
      "cost : 0.019816827028989792\n",
      "cost : 0.019816827028989792\n",
      "cost : 0.019816827028989792\n"
     ]
    }
   ],
   "source": [
    "# placeholder\n",
    "x = tf.placeholder(dtype=tf.float32)\n",
    "y = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "# training data set\n",
    "# 데이터 정제 필요!!! ## normalization : ( 요소값 - 최소값 ) / ( 최대값 - 최소값 )\n",
    "x_data = (df3[\"Temp\"]-df3[\"Temp\"].min())/(df3[\"Temp\"].max()-df3[\"Temp\"].min()) # Series 형태로 나옴\n",
    "y_data = (df3[\"Ozone\"]-df3[\"Ozone\"].min())/(df3[\"Ozone\"].max()-df3[\"Ozone\"].min())\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random_normal([1]), name = \"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name = \"bias\")\n",
    "\n",
    "# Hypothesis\n",
    "H = W * x + b\n",
    "\n",
    "# Cost Function\n",
    "cost = tf.reduce_mean(tf.square(H - y))\n",
    "\n",
    "# 최소화 노드생성\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Session 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습(train)\n",
    "for step in range(3000):\n",
    "    _, cost_val = sess.run([train,cost], feed_dict={x:x_data, y:y_data})\n",
    "    \n",
    "    if step % 300 == 0:\n",
    "        print(\"cost : {}\".format(cost_val)) # cost 값이 nan으로 발산하고 있음 => 값이 이상하게 나오는거..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ozone</th>\n",
       "      <th>Temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36.0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.0</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28.0</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23.0</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19.0</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7.0</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16.0</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11.0</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.0</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18.0</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14.0</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>34.0</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.0</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>30.0</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>11.0</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>11.0</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>32.0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>23.0</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>45.0</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>115.0</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>37.0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>29.0</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>71.0</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>39.0</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>23.0</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>85.0</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>96.0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>78.0</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>73.0</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>91.0</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>47.0</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>32.0</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>20.0</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>23.0</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>21.0</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>24.0</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>44.0</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>21.0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>28.0</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>9.0</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>13.0</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>46.0</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>18.0</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>13.0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>24.0</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>16.0</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>13.0</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>23.0</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>36.0</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>7.0</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>14.0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>30.0</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>14.0</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>18.0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>20.0</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Ozone  Temp\n",
       "0     41.0    67\n",
       "1     36.0    72\n",
       "2     12.0    74\n",
       "3     18.0    62\n",
       "5     28.0    66\n",
       "6     23.0    65\n",
       "7     19.0    59\n",
       "8      8.0    61\n",
       "10     7.0    74\n",
       "11    16.0    69\n",
       "12    11.0    66\n",
       "13    14.0    68\n",
       "14    18.0    58\n",
       "15    14.0    64\n",
       "16    34.0    66\n",
       "17     6.0    57\n",
       "18    30.0    68\n",
       "19    11.0    62\n",
       "20     1.0    59\n",
       "21    11.0    73\n",
       "22     4.0    61\n",
       "23    32.0    61\n",
       "27    23.0    67\n",
       "28    45.0    81\n",
       "29   115.0    79\n",
       "30    37.0    76\n",
       "37    29.0    82\n",
       "39    71.0    90\n",
       "40    39.0    87\n",
       "43    23.0    82\n",
       "..     ...   ...\n",
       "122   85.0    94\n",
       "123   96.0    91\n",
       "124   78.0    92\n",
       "125   73.0    93\n",
       "126   91.0    93\n",
       "127   47.0    87\n",
       "128   32.0    84\n",
       "129   20.0    80\n",
       "130   23.0    78\n",
       "131   21.0    75\n",
       "132   24.0    73\n",
       "133   44.0    81\n",
       "134   21.0    76\n",
       "135   28.0    77\n",
       "136    9.0    71\n",
       "137   13.0    71\n",
       "138   46.0    78\n",
       "139   18.0    67\n",
       "140   13.0    76\n",
       "141   24.0    68\n",
       "142   16.0    82\n",
       "143   13.0    64\n",
       "144   23.0    71\n",
       "145   36.0    81\n",
       "146    7.0    69\n",
       "147   14.0    63\n",
       "148   30.0    70\n",
       "150   14.0    75\n",
       "151   18.0    76\n",
       "152   20.0    68\n",
       "\n",
       "[116 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 0과 1 사이의 값으로 정규화 처리를 해줘야한다.. 그래야지 제대로된 학습이 가능하다.\n",
    "\n",
    "## normalization : ( 요소값 - 최소값 ) / ( 최대값 - 최소값 )\n",
    "## standardization : ( 요소값 - 평균 ) / 표준편차  # 음수값도 나올 수 도 있따\n",
    "df3\n",
    "## pandas를 이용해서 새로운 DataFrame생성해서 학습해보기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost : 3280.007080078125\n",
      "cost : nan\n",
      "cost : nan\n",
      "cost : nan\n",
      "cost : nan\n",
      "cost : nan\n",
      "cost : nan\n",
      "cost : nan\n",
      "cost : nan\n",
      "cost : nan\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 2차원형태 matrix\n",
    "# training data set\n",
    "\n",
    "x_data = [[73,80,75],\n",
    "          [93,88,93],\n",
    "          [89,91,90],\n",
    "          [96,96,100],\n",
    "          [73,66,70]]\n",
    "\n",
    "y_data = [[152],[185],[180],[196],[142]]\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,3], dtype=tf.float32) # shape=[5,3] : 5행 3열인 2차원배열(matrix)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32) # 행의 개수는 변할수 있기 때문에 None(상관하지 않겠다.don`t care의미)로 쓴다.\n",
    "\n",
    "# weight & bias\n",
    "W = tf.Variable(tf.random_normal([3,1]), name = \"weight\") # 행렬곱해서 W가 3행 1열 / matrix로 난수의 형태로 뽑을꺼임\n",
    "b = tf.Variable(tf.random_normal([1]), name = \"bias\")\n",
    "\n",
    "# Hypothesis\n",
    "# H = W * x + b\n",
    "H = tf.matmul(X,W) + b\n",
    "\n",
    "# Cost Function\n",
    "cost = tf.reduce_mean(tf.square(H - Y)) # square : 어떤데이터타입이 들어오든 알아서 계산해쥼\n",
    "\n",
    "# 학습노드 생성\n",
    "train  = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# for문\n",
    "for step in range(3000):\n",
    "    _, cost_val = sess.run([train,cost], feed_dict={X:x_data, Y:y_data})\n",
    "    \n",
    "    if step % 300 == 0:\n",
    "        print(\"cost : {}\".format(cost_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# x축 : \"Solar.R\",\"Wind\",\"Temp\"\n",
    "# y축 : \"Ozone\"\n",
    "## multiple linear regression \n",
    "## Ozone Data 학습 및 예측 \n",
    "\n",
    "# Fancy indexing\n",
    "df_x = df[[\"Solar.R\",\"Wind\",\"Temp\"]]\n",
    "display(df_x.shape)\n",
    "df_y = df[[\"Ozone\"]]\n",
    "display(df_y.shape)\n",
    "\n",
    "# 결측값 제거\n",
    "df4 = df_x.dropna(how=\"any\", inplace=False)\n",
    "display(df4.shape)\n",
    "df5 = df_y.dropna(how=\"any\", inplace=False)\n",
    "display(df5.shape)\n",
    "\n",
    "## training data set\n",
    "# => 데이터 정제 필요\n",
    "x_data = (df4[\"Solar.R\"]-)/()\n",
    "y_data = ()/()\n",
    "    \n",
    "## placeholder\n",
    "X = tf.placeholder(shape=[None,3], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "## weight & bias\n",
    "W = tf.Variable(tf.random_normal([]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([]), name=\"bias\")\n",
    "\n",
    "## Hypothesis\n",
    "H =  \n",
    "\n",
    "## Cost Function\n",
    "cost = tf.reduce_mean(tf.square())\n",
    "\n",
    "## 학습 노드 생성\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "## Session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost : 0.3413260877132416\n",
      "cost : 0.017036033794283867\n",
      "cost : 0.015741657465696335\n",
      "cost : 0.015542645007371902\n",
      "cost : 0.015511973761022091\n",
      "cost : 0.015507246367633343\n",
      "cost : 0.015506518073379993\n",
      "cost : 0.01550640631467104\n",
      "cost : 0.015506389550864697\n",
      "cost : 0.015506387688219547\n",
      "[[46.182]]\n"
     ]
    }
   ],
   "source": [
    "## 위에 문제 풀이\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler # 데이터 정제 => sklearn모듈 MinMaxScaler함수사용 : 정규화작업 \n",
    "\n",
    "# Data Loading\n",
    "df = pd.read_csv(\"./data/ozone/ozone.csv\", sep=\",\")\n",
    "\n",
    "# 필요한 컬럼만 추출\n",
    "df.drop([\"Month\",\"Day\"], axis=1, inplace=True) # drop은 컬럼과 로우 모두 지울수 있기때문에 axis를 줘야함 axis=1 열방향\n",
    "\n",
    "# 결측값 처리(제거)\n",
    "df.dropna(how=\"any\", inplace=True)\n",
    "\n",
    "# x 데이터 추출\n",
    "df_x = df.drop(\"Ozone\", axis=1, inplace=False)\n",
    "# display(df_x)\n",
    "# y 데이터 추출\n",
    "df_y = df[\"Ozone\"]\n",
    "# display(df_y) # Series로 떨어짐 => values\n",
    "\n",
    "# training data set\n",
    "x_data = MinMaxScaler().fit_transform(df_x.values)\n",
    "y_data = MinMaxScaler().fit_transform(df_y.values.reshape([-1,1])) # 데이터를 1열로 만들어서 2차원형태로 변경\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,3], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random_normal([3,1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# Hypothesis\n",
    "H = tf.matmul(X,W) + b\n",
    "\n",
    "# Cost Function\n",
    "cost = tf.reduce_mean(tf.square(H - Y))\n",
    "\n",
    "# train node 생성\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Session & 초기화 \n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습진행 - 반복 - for문\n",
    "for step in range(30000):\n",
    "    _, cost_val = sess.run([train,cost], feed_dict={X:x_data, Y:y_data})\n",
    "    \n",
    "    if step % 3000 == 0:\n",
    "        print(\"cost : {}\".format(cost_val))\n",
    "        \n",
    "# prediction\n",
    "print(sess.run(H, feed_dict={X:[[190,7.4,67]]})) # placeholder에 정의된 shape를 따라줘야함 / 190,7.4,67,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.50497\n",
      "0.04353742\n",
      "0.04353742\n",
      "0.04353742\n",
      "0.04353742\n",
      "0.04353742\n",
      "0.04353742\n",
      "0.04353742\n",
      "0.04353742\n",
      "0.04353742\n",
      "[0.50340176]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 2019-07-12\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt #그래프 그릴때 사용\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "# training data set\n",
    "x_data = [1,2,5,8,10]\n",
    "y_data = [0,0,0,1,1]\n",
    "\n",
    "# placeholder\n",
    "x = tf.placeholder(dtype=tf.float32)\n",
    "y = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random_normal([1]), name=\"weight\") \n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# Hypothesis(스칼라 연산)\n",
    "H = W * x + b\n",
    "\n",
    "# cost function \n",
    "cost = tf.reduce_mean(tf.square(H-y))\n",
    "\n",
    "# cost -2차함수 -> 미분이용 -> 최소값 찾기\n",
    "#train node생성\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습과정 진행\n",
    "for step in range(30000):\n",
    "    _,cost_val = sess.run([train,cost], feed_dict={x:x_data,y:y_data})\n",
    "    if(step % 3000 == 0):\n",
    "        print(cost_val)\n",
    "\n",
    "# prediction\n",
    "print(sess.run(H, feed_dict = {x:[6]}))\n",
    "\n",
    "# plot\n",
    "plt.scatter(x_data,y_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "plt.scatter(x_data,y_data)\n",
    "plt.plot(x_data,x_data * sess.run(W) + sess.run(b),\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.577001\n",
      "0.2753701\n",
      "0.21381187\n",
      "0.18668908\n",
      "0.16951975\n",
      "0.15678768\n",
      "0.14653523\n",
      "0.13787729\n",
      "0.13034703\n",
      "0.12366992\n",
      "정확도:1.0\n",
      "[[0.81499165]]\n"
     ]
    }
   ],
   "source": [
    "#Sigmoid function 예제\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt #그래프 그릴때 사용\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "# training data set\n",
    "x_data = [[30,0],\n",
    "         [10,0],\n",
    "         [8,1],\n",
    "         [3,3],\n",
    "         [2,3],\n",
    "         [5,1],\n",
    "         [2,0],\n",
    "         [1,0]]\n",
    "y_data = [[1],[1],[1],[1],[1],[0],[0],[0]]\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2],dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1],dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random_normal([2,1]),name=\"weight\") #행열곱한 값 :[2,1]\n",
    "b = tf.Variable(tf.random_normal([1]),name=\"bias\") \n",
    "\n",
    "# Hypothesis\n",
    "logits = tf.matmul(X,W) + b\n",
    "H = tf.sigmoid(logits) #sigmoid - 직선을 곡선 형태로 변경\n",
    "\n",
    "# cost function\n",
    "# sigmoid_cross_entropy_with_logits -GradientDescent 사용가능하게 변경\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "\n",
    "# training node 생성\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습과정 진행\n",
    "for step in range(30000):\n",
    "    _,cost_val = sess.run([train,cost], feed_dict={X:x_data,Y:y_data})\n",
    "    if(step % 3000 == 0):\n",
    "        print(cost_val)\n",
    "\n",
    "# Accuracy (정확도 측정)\n",
    "# 97~98%정도 나와줘야함\n",
    "# boolean값을 실수로 변경\n",
    "# H > 0.5 크면 1로 지정\n",
    "predict = tf.cast(H > 0.5, dtype=tf.float32) #예측값\n",
    "correct = tf.equal(predict, Y) #예측값과 실제값이 같으면 1(True)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32)) #정확도 측정\n",
    "print(\"정확도:{}\".format(sess.run(accuracy,\n",
    "                              feed_dict={X:x_data,Y:y_data})))\n",
    "# 정확도:1.0 => 100%\n",
    "# (최소값 0, 최대값 1)\n",
    "# =>정확도 측정은 평가 데이터(Test)로 평가해야함\n",
    "# 가지고 있는 학습데이터셋을 7:3으로 나눠서 학습과 평가를 해야함\n",
    "\n",
    "# prediction\n",
    "print(sess.run(H, feed_dict = {X:[[4,2]]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>2.93</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>760</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>560</td>\n",
       "      <td>2.98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "      <td>3.08</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>540</td>\n",
       "      <td>3.39</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>700</td>\n",
       "      <td>3.92</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>800</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>440</td>\n",
       "      <td>3.22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>760</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>700</td>\n",
       "      <td>3.08</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>700</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>480</td>\n",
       "      <td>3.44</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>780</td>\n",
       "      <td>3.87</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>360</td>\n",
       "      <td>2.56</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>800</td>\n",
       "      <td>3.75</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>540</td>\n",
       "      <td>3.81</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>3.17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>660</td>\n",
       "      <td>3.63</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>600</td>\n",
       "      <td>2.82</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>680</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>760</td>\n",
       "      <td>3.35</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>3.66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>620</td>\n",
       "      <td>3.61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>520</td>\n",
       "      <td>3.74</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>780</td>\n",
       "      <td>3.22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>3.29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>1</td>\n",
       "      <td>540</td>\n",
       "      <td>3.77</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>1</td>\n",
       "      <td>680</td>\n",
       "      <td>3.76</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>1</td>\n",
       "      <td>680</td>\n",
       "      <td>2.42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>1</td>\n",
       "      <td>620</td>\n",
       "      <td>3.37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>0</td>\n",
       "      <td>560</td>\n",
       "      <td>3.78</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>0</td>\n",
       "      <td>560</td>\n",
       "      <td>3.49</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>0</td>\n",
       "      <td>620</td>\n",
       "      <td>3.63</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>0</td>\n",
       "      <td>640</td>\n",
       "      <td>3.12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>0</td>\n",
       "      <td>700</td>\n",
       "      <td>3.65</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>1</td>\n",
       "      <td>540</td>\n",
       "      <td>3.49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>3.51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>0</td>\n",
       "      <td>660</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>1</td>\n",
       "      <td>480</td>\n",
       "      <td>2.62</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>0</td>\n",
       "      <td>420</td>\n",
       "      <td>3.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>1</td>\n",
       "      <td>740</td>\n",
       "      <td>3.86</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>0</td>\n",
       "      <td>580</td>\n",
       "      <td>3.36</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>0</td>\n",
       "      <td>640</td>\n",
       "      <td>3.17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>0</td>\n",
       "      <td>640</td>\n",
       "      <td>3.51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>3.05</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>1</td>\n",
       "      <td>660</td>\n",
       "      <td>3.88</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>1</td>\n",
       "      <td>600</td>\n",
       "      <td>3.38</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>1</td>\n",
       "      <td>620</td>\n",
       "      <td>3.75</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>1</td>\n",
       "      <td>460</td>\n",
       "      <td>3.99</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0</td>\n",
       "      <td>620</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0</td>\n",
       "      <td>560</td>\n",
       "      <td>3.04</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0</td>\n",
       "      <td>460</td>\n",
       "      <td>2.63</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0</td>\n",
       "      <td>700</td>\n",
       "      <td>3.65</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0</td>\n",
       "      <td>600</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     admit  gre   gpa  rank\n",
       "0        0  380  3.61     3\n",
       "1        1  660  3.67     3\n",
       "2        1  800  4.00     1\n",
       "3        1  640  3.19     4\n",
       "4        0  520  2.93     4\n",
       "5        1  760  3.00     2\n",
       "6        1  560  2.98     1\n",
       "7        0  400  3.08     2\n",
       "8        1  540  3.39     3\n",
       "9        0  700  3.92     2\n",
       "10       0  800  4.00     4\n",
       "11       0  440  3.22     1\n",
       "12       1  760  4.00     1\n",
       "13       0  700  3.08     2\n",
       "14       1  700  4.00     1\n",
       "15       0  480  3.44     3\n",
       "16       0  780  3.87     4\n",
       "17       0  360  2.56     3\n",
       "18       0  800  3.75     2\n",
       "19       1  540  3.81     1\n",
       "20       0  500  3.17     3\n",
       "21       1  660  3.63     2\n",
       "22       0  600  2.82     4\n",
       "23       0  680  3.19     4\n",
       "24       1  760  3.35     2\n",
       "25       1  800  3.66     1\n",
       "26       1  620  3.61     1\n",
       "27       1  520  3.74     4\n",
       "28       1  780  3.22     2\n",
       "29       0  520  3.29     1\n",
       "..     ...  ...   ...   ...\n",
       "370      1  540  3.77     2\n",
       "371      1  680  3.76     3\n",
       "372      1  680  2.42     1\n",
       "373      1  620  3.37     1\n",
       "374      0  560  3.78     2\n",
       "375      0  560  3.49     4\n",
       "376      0  620  3.63     2\n",
       "377      1  800  4.00     2\n",
       "378      0  640  3.12     3\n",
       "379      0  540  2.70     2\n",
       "380      0  700  3.65     2\n",
       "381      1  540  3.49     2\n",
       "382      0  540  3.51     2\n",
       "383      0  660  4.00     1\n",
       "384      1  480  2.62     2\n",
       "385      0  420  3.02     1\n",
       "386      1  740  3.86     2\n",
       "387      0  580  3.36     2\n",
       "388      0  640  3.17     2\n",
       "389      0  640  3.51     2\n",
       "390      1  800  3.05     2\n",
       "391      1  660  3.88     2\n",
       "392      1  600  3.38     3\n",
       "393      1  620  3.75     2\n",
       "394      1  460  3.99     3\n",
       "395      0  620  4.00     2\n",
       "396      0  560  3.04     3\n",
       "397      0  460  2.63     2\n",
       "398      0  700  3.65     2\n",
       "399      0  600  3.89     3\n",
       "\n",
       "[400 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.66163224\n",
      "0.5834495\n",
      "0.56327796\n",
      "0.5563769\n",
      "0.5536019\n",
      "0.55231977\n",
      "0.5516539\n",
      "0.5512747\n",
      "0.5510435\n",
      "0.55089617\n",
      "정확도:0.6583333611488342\n"
     ]
    }
   ],
   "source": [
    "# 실습 - 점수에 따른 대학원 입학여부 추측\n",
    "# ./data/admission/admission.csv 대학원 입학여부 데이터\n",
    "# admit :입학여부 (1:입학 0:입학불가), gre, gpa, rank :등급\n",
    "# 7:3으로 데이터 나눠서 테스트(7: 학습용,3: 테스트용으로 사용)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#data loading\n",
    "df = pd.read_csv(\"./data/admission/admission.csv\",sep=\",\")\n",
    "# display(df.head())\n",
    "df.dropna(how=\"any\", inplace=True) \n",
    "\n",
    "step_cnt = int(df.shape[0] * 0.7)\n",
    "df_x = df.loc[:step_cnt,[\"gre\",\"gpa\",\"rank\"]]\n",
    "df_y = df.loc[:step_cnt,[\"admit\"]]\n",
    "\n",
    "#training data set\n",
    "x_data = MinMaxScaler().fit_transform(df_x.values)\n",
    "y_data = MinMaxScaler().fit_transform(df_y.values.reshape(-1,1))\n",
    "\n",
    "#placeholder\n",
    "X = tf.placeholder(shape=[None,3],dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1],dtype=tf.float32)\n",
    "\n",
    "#Weight & bias\n",
    "W = tf.Variable(tf.random_normal([3,1]),name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]),name=\"bias\")\n",
    "\n",
    "#Hypothesis\n",
    "logits = tf.matmul(X,W) + b\n",
    "H = tf.sigmoid(logits) #sigmoid - 직선을 곡선 형태로 변경\n",
    "\n",
    "#cost function\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "\n",
    "#training node 생성\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "#session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#학습과정 진행\n",
    "for step in range(30000):\n",
    "    _,cost_val = sess.run([train,cost], feed_dict={X:x_data,Y:y_data})\n",
    "    if(step % 3000 == 0):\n",
    "        print(cost_val)\n",
    "\n",
    "#Accuracy (정확도 측정)\n",
    "df_x = df.loc[step_cnt:,[\"gre\",\"gpa\",\"rank\"]]\n",
    "df_y = df.loc[step_cnt:,[\"admit\"]]\n",
    "\n",
    "x_data_t = MinMaxScaler().fit_transform(df_x.values)\n",
    "y_data_t = MinMaxScaler().fit_transform(df_y.values.reshape(-1,1))\n",
    "\n",
    "predict = tf.cast(H > 0.5, dtype=tf.float32) #예측값\n",
    "correct = tf.equal(predict, Y) #예측값과 실제값이 같으면 1(True)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32)) #정확도 측정\n",
    "print(\"정확도:{}\".format(sess.run(accuracy,\n",
    "                              feed_dict={X:x_data_t,Y:y_data_t})))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Sandstrom, Miss. Marguerite Rut</td>\n",
       "      <td>female</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PP 9549</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>G6</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bonnell, Miss. Elizabeth</td>\n",
       "      <td>female</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113783</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>C103</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Saundercock, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5. 2151</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Andersson, Mr. Anders Johan</td>\n",
       "      <td>male</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>347082</td>\n",
       "      <td>31.2750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Vestrom, Miss. Hulda Amanda Adolfina</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>350406</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Hewlett, Mrs. (Mary D Kingcome)</td>\n",
       "      <td>female</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>248706</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Rice, Master. Eugene</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>382652</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Williams, Mr. Charles Eugene</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>244373</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Vander Planke, Mrs. Julius (Emelia Maria Vande...</td>\n",
       "      <td>female</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>345763</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Masselmani, Mrs. Fatima</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2649</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Fynney, Mr. Joseph J</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>239865</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PassengerId  Survived  Pclass  \\\n",
       "0             1         0       3   \n",
       "1             2         1       1   \n",
       "2             3         1       3   \n",
       "3             4         1       1   \n",
       "4             5         0       3   \n",
       "5             6         0       3   \n",
       "6             7         0       1   \n",
       "7             8         0       3   \n",
       "8             9         1       3   \n",
       "9            10         1       2   \n",
       "10           11         1       3   \n",
       "11           12         1       1   \n",
       "12           13         0       3   \n",
       "13           14         0       3   \n",
       "14           15         0       3   \n",
       "15           16         1       2   \n",
       "16           17         0       3   \n",
       "17           18         1       2   \n",
       "18           19         0       3   \n",
       "19           20         1       3   \n",
       "20           21         0       2   \n",
       "\n",
       "                                                 Name     Sex   Age  SibSp  \\\n",
       "0                             Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1   Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                              Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3        Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                            Allen, Mr. William Henry    male  35.0      0   \n",
       "5                                    Moran, Mr. James    male   NaN      0   \n",
       "6                             McCarthy, Mr. Timothy J    male  54.0      0   \n",
       "7                      Palsson, Master. Gosta Leonard    male   2.0      3   \n",
       "8   Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
       "9                 Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
       "10                    Sandstrom, Miss. Marguerite Rut  female   4.0      1   \n",
       "11                           Bonnell, Miss. Elizabeth  female  58.0      0   \n",
       "12                     Saundercock, Mr. William Henry    male  20.0      0   \n",
       "13                        Andersson, Mr. Anders Johan    male  39.0      1   \n",
       "14               Vestrom, Miss. Hulda Amanda Adolfina  female  14.0      0   \n",
       "15                   Hewlett, Mrs. (Mary D Kingcome)   female  55.0      0   \n",
       "16                               Rice, Master. Eugene    male   2.0      4   \n",
       "17                       Williams, Mr. Charles Eugene    male   NaN      0   \n",
       "18  Vander Planke, Mrs. Julius (Emelia Maria Vande...  female  31.0      1   \n",
       "19                            Masselmani, Mrs. Fatima  female   NaN      0   \n",
       "20                               Fynney, Mr. Joseph J    male  35.0      0   \n",
       "\n",
       "    Parch            Ticket     Fare Cabin Embarked  \n",
       "0       0         A/5 21171   7.2500   NaN        S  \n",
       "1       0          PC 17599  71.2833   C85        C  \n",
       "2       0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3       0            113803  53.1000  C123        S  \n",
       "4       0            373450   8.0500   NaN        S  \n",
       "5       0            330877   8.4583   NaN        Q  \n",
       "6       0             17463  51.8625   E46        S  \n",
       "7       1            349909  21.0750   NaN        S  \n",
       "8       2            347742  11.1333   NaN        S  \n",
       "9       0            237736  30.0708   NaN        C  \n",
       "10      1           PP 9549  16.7000    G6        S  \n",
       "11      0            113783  26.5500  C103        S  \n",
       "12      0         A/5. 2151   8.0500   NaN        S  \n",
       "13      5            347082  31.2750   NaN        S  \n",
       "14      0            350406   7.8542   NaN        S  \n",
       "15      0            248706  16.0000   NaN        S  \n",
       "16      1            382652  29.1250   NaN        Q  \n",
       "17      0            244373  13.0000   NaN        S  \n",
       "18      0            345763  18.0000   NaN        S  \n",
       "19      0              2649   7.2250   NaN        C  \n",
       "20      0            239865  26.0000   NaN        S  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 실습 - 타이타닉 구조 현황 데이터 학습,평가\n",
    "#./data/titanic/train.csv\n",
    "#PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked\n",
    "#PassengerId 탑승자id,Survived 구조여부 (1 구조 0 구조못함)\n",
    "#Pclass 좌석등급, Name 이름, Sex 성별, Age 나이,SibSp 형제수,Parch 부모/자식수,\n",
    "#Ticket 티켓번호,Fare 요금, Cabin 객실, Embarked 탑승한 곳\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#data loading\n",
    "df = pd.read_csv(\"./data/titanic/train.csv\",sep=\",\")\n",
    "\n",
    "display(df.loc[:20,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>51.8625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>21.0750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11.1333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>30.0708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16.7000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>31.2750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7.8542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>29.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>26.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>35.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>21.0750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>31.3875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>263.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>27.7208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>82.1708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>52.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>164.8667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>26.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>19.2583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>14.1083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.9292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13.8583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50.4958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11.1333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.8958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>52.5542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>24.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7.2250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9.8458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.8958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>83.1583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.8958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10.5167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>29.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>30.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>30.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.7500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>714 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass Sex Age      Fare\n",
       "0         3   1   2    7.2500\n",
       "1         1   2   2   71.2833\n",
       "2         3   2   2    7.9250\n",
       "3         1   2   2   53.1000\n",
       "4         3   1   2    8.0500\n",
       "6         1   1   2   51.8625\n",
       "7         3   1   2   21.0750\n",
       "8         3   2   2   11.1333\n",
       "9         2   2   2   30.0708\n",
       "10        3   2   2   16.7000\n",
       "11        1   2   2   26.5500\n",
       "12        3   1   2    8.0500\n",
       "13        3   1   2   31.2750\n",
       "14        3   2   2    7.8542\n",
       "15        2   2   2   16.0000\n",
       "16        3   1   2   29.1250\n",
       "18        3   2   2   18.0000\n",
       "20        2   1   2   26.0000\n",
       "21        2   1   2   13.0000\n",
       "22        3   2   2    8.0292\n",
       "23        1   1   2   35.5000\n",
       "24        3   2   2   21.0750\n",
       "25        3   2   2   31.3875\n",
       "27        1   1   2  263.0000\n",
       "30        1   1   2   27.7208\n",
       "33        2   1   2   10.5000\n",
       "34        1   1   2   82.1708\n",
       "35        1   1   2   52.0000\n",
       "37        3   1   2    8.0500\n",
       "38        3   2   2   18.0000\n",
       "..      ...  ..  ..       ...\n",
       "856       1   2   2  164.8667\n",
       "857       1   1   2   26.5500\n",
       "858       3   2   2   19.2583\n",
       "860       3   1   2   14.1083\n",
       "861       2   1   2   11.5000\n",
       "862       1   2   2   25.9292\n",
       "864       2   1   2   13.0000\n",
       "865       2   2   2   13.0000\n",
       "866       2   2   2   13.8583\n",
       "867       1   1   2   50.4958\n",
       "869       3   1   2   11.1333\n",
       "870       3   1   2    7.8958\n",
       "871       1   2   2   52.5542\n",
       "872       1   1   2    5.0000\n",
       "873       3   1   2    9.0000\n",
       "874       2   2   2   24.0000\n",
       "875       3   2   2    7.2250\n",
       "876       3   1   2    9.8458\n",
       "877       3   1   2    7.8958\n",
       "879       1   2   2   83.1583\n",
       "880       2   2   2   26.0000\n",
       "881       3   1   2    7.8958\n",
       "882       3   2   2   10.5167\n",
       "883       2   1   2   10.5000\n",
       "884       3   1   2    7.0500\n",
       "885       3   2   2   29.1250\n",
       "886       2   1   2   13.0000\n",
       "887       1   2   2   30.0000\n",
       "889       1   1   2   30.0000\n",
       "890       3   1   2    7.7500\n",
       "\n",
       "[714 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df[[\"Pclass\",\"Sex\",\"Age\",\"Fare\",\"Survived\"]]\n",
    "df2.dropna(how=\"any\", inplace=True)\n",
    "\n",
    "df_x = df2[[\"Pclass\",\"Sex\",\"Age\",\"Fare\"]]\n",
    "df_y = df2[[\"Survived\"]]\n",
    "#print(df_x.shape)\n",
    "#print(df_y.shape)\n",
    "df_x[\"Sex\"] = df2['Sex'].apply(lambda x: '1' if x == 'male' else '2')\n",
    "df_x[\"Age\"] = df2['Age'].apply(lambda x: '1' if x == 'male' else '2')\n",
    "df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.91825\n",
      "0.33846167\n",
      "0.25144625\n",
      "0.19828959\n",
      "0.16248058\n",
      "0.1369248\n",
      "0.11789898\n",
      "0.10325836\n",
      "0.09168744\n",
      "0.08233913\n",
      "Accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "## Multinomal Classification \n",
    "import tensorflow as tf\n",
    "\n",
    "# training data set\n",
    "x_data = [[10,7,8,5],\n",
    "         [8,8,9,4],\n",
    "         [7,8,2,3],\n",
    "         [6,3,9,3],\n",
    "         [7,5,7,4],\n",
    "         [3,5,6,2],\n",
    "         [2,4,3,1]]\n",
    "\n",
    "y_data = [[1,0,0],     # one-hot encoding\n",
    "          [1,0,0],\n",
    "          [0,1,0],\n",
    "          [0,1,0],\n",
    "          [0,1,0],\n",
    "          [0,0,1],\n",
    "          [0,0,1]]\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,4], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,3], dtype=tf.float32)\n",
    "\n",
    "# weight & bias\n",
    "W = tf.Variable(tf.random_normal([4,3]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([3]), name=\"bias\")\n",
    "\n",
    "# Hypothesis\n",
    "logits = tf.matmul(X,W) + b\n",
    "H = tf.nn.softmax(logits)\n",
    "\n",
    "# Cost Function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "\n",
    "# train node 생성\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "# Session & 초기화 \n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습진행 - 반복 - for문\n",
    "for step in range(30000):\n",
    "    _, cost_val = sess.run([train,cost], feed_dict={X:x_data, Y:y_data})\n",
    "    \n",
    "    if step % 3000 == 0:\n",
    "        print(cost_val)\n",
    "\n",
    "# Accuracy 정확도 측정\n",
    "# logistic => H가 0~1사이의 실수로 값 산출\n",
    "# multinomial => (확률, 확률, 확률)\n",
    "#  예)           (0.4, 0.5, 0.1)  => 1\n",
    "pridict = tf.arg_max(H,1) # arg_max() : 어느위치에 있는 확률이 큰지 \n",
    "correct = tf.equal(pridict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "print(\"Accuracy : {}\".format(sess.run(accuracy, feed_dict={X:x_data, Y:y_data})))\n",
    "# prediction 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>188</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>161</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>178</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>136</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>123</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>169</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>154</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>136</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>152</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>131</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>197</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>142</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>157</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>127</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>148</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>179</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>186</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19970</th>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19971</th>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19972</th>\n",
       "      <td>2</td>\n",
       "      <td>149</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19973</th>\n",
       "      <td>0</td>\n",
       "      <td>196</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19974</th>\n",
       "      <td>2</td>\n",
       "      <td>137</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19975</th>\n",
       "      <td>2</td>\n",
       "      <td>149</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19976</th>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19977</th>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19978</th>\n",
       "      <td>1</td>\n",
       "      <td>190</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19979</th>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19980</th>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19981</th>\n",
       "      <td>1</td>\n",
       "      <td>149</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19982</th>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19983</th>\n",
       "      <td>1</td>\n",
       "      <td>167</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19984</th>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19985</th>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19986</th>\n",
       "      <td>0</td>\n",
       "      <td>198</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19987</th>\n",
       "      <td>1</td>\n",
       "      <td>134</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19988</th>\n",
       "      <td>1</td>\n",
       "      <td>196</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19989</th>\n",
       "      <td>2</td>\n",
       "      <td>131</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19990</th>\n",
       "      <td>2</td>\n",
       "      <td>131</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19991</th>\n",
       "      <td>0</td>\n",
       "      <td>191</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19992</th>\n",
       "      <td>1</td>\n",
       "      <td>168</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19993</th>\n",
       "      <td>2</td>\n",
       "      <td>136</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19994</th>\n",
       "      <td>0</td>\n",
       "      <td>194</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>0</td>\n",
       "      <td>163</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>2</td>\n",
       "      <td>139</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>1</td>\n",
       "      <td>189</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>1</td>\n",
       "      <td>142</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  height  weight\n",
       "0          1     188      71\n",
       "1          2     161      68\n",
       "2          0     178      52\n",
       "3          2     136      63\n",
       "4          1     145      52\n",
       "5          2     123      45\n",
       "6          2     135      59\n",
       "7          0     169      45\n",
       "8          2     120      46\n",
       "9          2     154      75\n",
       "10         2     136      60\n",
       "11         0     158      46\n",
       "12         1     132      41\n",
       "13         2     152      59\n",
       "14         2     131      53\n",
       "15         1     165      59\n",
       "16         0     188      55\n",
       "17         2     129      43\n",
       "18         2     122      61\n",
       "19         1     197      80\n",
       "20         2     142      72\n",
       "21         0     168      38\n",
       "22         0     157      41\n",
       "23         2     153      67\n",
       "24         0     168      47\n",
       "25         1     127      39\n",
       "26         1     148      47\n",
       "27         0     160      40\n",
       "28         0     179      59\n",
       "29         1     186      73\n",
       "...      ...     ...     ...\n",
       "19970      2     135      59\n",
       "19971      1     132      37\n",
       "19972      2     149      68\n",
       "19973      0     196      65\n",
       "19974      2     137      76\n",
       "19975      2     149      69\n",
       "19976      1     158      48\n",
       "19977      2     120      61\n",
       "19978      1     190      76\n",
       "19979      0     148      37\n",
       "19980      2     130      67\n",
       "19981      1     149      42\n",
       "19982      1     137      40\n",
       "19983      1     167      52\n",
       "19984      2     129      76\n",
       "19985      2     153      75\n",
       "19986      0     198      72\n",
       "19987      1     134      43\n",
       "19988      1     196      77\n",
       "19989      2     131      77\n",
       "19990      2     131      72\n",
       "19991      0     191      54\n",
       "19992      1     168      61\n",
       "19993      2     136      75\n",
       "19994      0     194      51\n",
       "19995      0     163      48\n",
       "19996      2     139      70\n",
       "19997      1     150      48\n",
       "19998      1     189      69\n",
       "19999      1     142      41\n",
       "\n",
       "[20000 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.one_hot(컬럼,3) : 컬럼값을 3개로 분할하고\n",
    "# Logistic => titanic\n",
    "# Multinomial => BMI\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Data Loading\n",
    "df_bmi = pd.read_csv(\"./data/BMI/bmi.csv\", sep=\",\", skiprows=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0.]],\n",
       "\n",
       "       [[0., 0., 1.]]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tf.one_hot([[0],[1],[2]],3)\n",
    "sess = tf.Session()\n",
    "test = sess.run(test)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine learning의 3가지 분류\n",
    "# 1. supervised learning(지도학습)\n",
    "#  => training data에 label이 부여되어 있다\n",
    "# 2. unsupervised learning(비지도 학습)\n",
    "#  => traing data에 lable이 존재하지 않음\n",
    "#  => clustering작없이 일반적으로 진행\n",
    "# 3. 강화학습\n",
    "#  => 상점과 벌점을 이용하여 점점 더 좋은 방향으로 학습해 나가는 방식\n",
    "\n",
    "# Supervise Learning(지도학습)\n",
    "# 1. single linear regression(단순 선형회귀) - x 한개, y 한개 존재하는 경우\n",
    "# 2. multiple linear regression(다중 선형회귀)\n",
    "#  => multrix\n",
    "# 3. logistic regression(binary classification) -둘중에 한개의 값으로 판단\n",
    "# 4. Multinomial classification - 여러개 중에 한개의 값으로 판단\n",
    "\n",
    "# 추가적으로 알아야 할 내용\n",
    "# Large learning rate 인 경우\n",
    "#  Overshooting - W값 ,값이 클경우 이동 거리가 큼\n",
    "# Small learing rate 인 경우\n",
    "#  local minimum문제에 봉착할 수 있음(값을 못찾는 문제)\n",
    "\n",
    "# learning rate- 0.01값으로 시작\n",
    "# 입력데이터의 값이 범위가 클 경우 \n",
    "# - 학습진행이 되지 않는다\n",
    "# - normalization (정규화),standardization (표준화) 진행되어야 함. 일반적으로 normalization 사용됨\n",
    "\n",
    "# Overfitting\n",
    "# -training data set에는 아주 적합한 모델이지만 실제 데어터에는 적용이 안되는 경우\n",
    "# Evaluation\n",
    "# - training data와 test data 분리하여 평가(8:2, 7:3)\n",
    "\n",
    "# MNIST\n",
    "# 예) 우편번호를 스캔하여 읽어들인 숫자 하나하나 픽셀값을 학습시켜서 \n",
    "# 새로운 값이 들어왔을때 예측을 시킴\n",
    "# 숫자에 대한 픽셀정보(28*28)를 학습\n",
    "# =>이 픽셀정보가 어떤숫자인지 알려주는 label도 제공(0~9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "55000\n",
      "(55000, 784)\n",
      "(55000, 10)\n",
      "4.1983714\n",
      "2.1391783\n",
      "1.6185739\n",
      "1.6010695\n",
      "1.0683154\n",
      "0.84826386\n",
      "0.7693103\n",
      "1.0302414\n",
      "0.68998885\n",
      "1.090036\n",
      "정확도 :0.8335999846458435\n"
     ]
    }
   ],
   "source": [
    "## 기본 MNIST 예제 (multinomial classification)\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Data Loading \n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True) # 폴더를 만들고 데이터 저장 , 압축파일 파일 압축풀 필요없음\n",
    "\n",
    "# 데이터 확인\n",
    "print(mnist.train.num_examples) # 학습용 데이터의 개수 / 학습용 = train , 테스트용 = \n",
    "print(mnist.train.images.shape) # (55000, 784)\n",
    "                                #  28 X 28 이미지를 1차원 형태로 저장\n",
    "print(mnist.train.labels.shape)\n",
    "\n",
    "# plt.imshow(mnist.train.images[0].reshape(28,28), cmap=\"Greys\", interpolation=\"nearest\") # 1차원데이터를 2차원데이터로 변경\n",
    "# # 55000개의 그림중에 이미지를 선택해서 픽셀정보를 줘야한다.\n",
    "# plt.show()\n",
    "# print(mnist.train.labels[0])\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None, 784], dtype=tf.float32) \n",
    "Y = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random_normal([784,10]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([10]), name=\"bias\")\n",
    "\n",
    "# Hypothesis / 각각에 대한 확률을 구해야한다.\n",
    "logits = tf.matmul(X,W)+b  # 행렬곱을 기본으로 잡는다.\n",
    "H = tf.nn.softmax(logits)\n",
    "\n",
    "# Cost Function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "\n",
    "# train node 생성\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "# train = optimizer.minimize(cost)\n",
    "\n",
    "# Session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\"\"\" \n",
    "사용하는 데이터의 크기가 상당히 크다.\n",
    "메모리의 문제때문에 한번에 불러올수 없다.\n",
    "데이터의 크기에 상관없이 학습하는 방식이 필요하다.!! => 잘라서 학습하는 방식으로 수행\n",
    "epoch : training data를 1번 학습시키는 것.\n",
    "\"\"\"\n",
    "# epoch 학습진행\n",
    "training_epoch = 30 # for 루프를 30만큼 돌린다는 의미\n",
    "batch_size = 100 #  55000개의 행을 다 읽어들이는게 아니라 100개의 행을 읽어서 반복학습!! (아중루프가 돔) / 얼마만큼의 사이즈로 불러드릴껀지\n",
    "\n",
    "for step in range(training_epoch): # 30 epoch 만큼 반복\n",
    "    num_of_iter = int(mnist.train.num_examples /  batch_size) # 550번 끊어 읽겠다는 의미\n",
    "    cost_val = 0\n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size) \n",
    "        _, cost_val = sess.run([train,cost], feed_dict={X:batch_x , Y:batch_y})\n",
    "        \n",
    "    if step % 3 == 0:\n",
    "        print(cost_val)     \n",
    "\n",
    "# 학습진행\n",
    "# for step in range(3000):\n",
    "#     _, cost_val = sess.run([train,cost], feed_dict={X:mnist.train.images, Y:mnist.train.labels})\n",
    "    \n",
    "#     if step % 300 == 0:\n",
    "#         print(cost_val)\n",
    "\n",
    "# Accuracy(정확도) 측정\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "result = sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels})\n",
    "\n",
    "print(\"정확도 :{}\".format(result))\n",
    "\n",
    "# Prediction\n",
    "## 종이에 숫자를 하나 써서 스캐너로 읽어들인 후 28*28의 형태의 픽셀 데이터로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7288799\n",
      "0.63950753\n",
      "0.5713611\n",
      "0.51655877\n",
      "0.47173175\n",
      "0.43456194\n",
      "0.40331703\n",
      "0.37670112\n",
      "0.35374552\n",
      "0.33372247\n",
      "정확도 : [1.0]\n"
     ]
    }
   ],
   "source": [
    "## Deep Learning\n",
    "\n",
    "# logistic regression을 이용하여 AND 연산을 학습\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# training data set\n",
    "x_data = [[0,0],\n",
    "          [0,1],\n",
    "          [1,0],\n",
    "          [1,1]]\n",
    "\n",
    "y_data = [[0],[0],[0],[1]]\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random_normal([2,1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# Hypothesis\n",
    "logits = tf.matmul(X,W) + b\n",
    "H = tf.sigmoid(logits)\n",
    "\n",
    "# Cost Function\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "\n",
    "# train node\n",
    "# train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(3000):\n",
    "    _, cost_val = sess.run([train,cost], feed_dict={X:x_data, Y:y_data})\n",
    "        \n",
    "    if step % 300 == 0:\n",
    "        print(cost_val)     \n",
    "        \n",
    "# Accuracy 측정 \n",
    "predict = tf.cast(H > 0.5, dtype=tf.float32)\n",
    "correct = tf.equal(predict,Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "print(\"정확도 : {}\".format(sess.run([accuracy], feed_dict={X:x_data, Y:y_data})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8809452\n",
      "0.6462946\n",
      "0.5802996\n",
      "0.4986219\n",
      "0.39928943\n",
      "0.29050106\n",
      "0.20047064\n",
      "0.13998869\n",
      "0.101986125\n",
      "0.07773567\n",
      "정확도 : [1.0]\n"
     ]
    }
   ],
   "source": [
    "## Deep Learning\n",
    "\n",
    "# NN(Neural Network)을 이용하여 XOR 연산을 학습\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# training data set( XOR에 대한 진리표)\n",
    "x_data = [[0,0],\n",
    "          [0,1],\n",
    "          [1,0],\n",
    "          [1,1]]\n",
    "\n",
    "y_data = [[0],[1],[1],[0]]\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "# 첫번째 레이어\n",
    "W1 = tf.Variable(tf.random_normal([2,8]), name=\"weight1\") # ([2,?]) : ? : 두번째레이어의 몇개의 input을 사용할껀지\n",
    "b1 = tf.Variable(tf.random_normal([8]), name=\"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1) + b1)\n",
    "\n",
    "# 두번째 레이어\n",
    "W2 = tf.Variable(tf.random_normal([8,1]), name=\"weight2\") \n",
    "b2 = tf.Variable(tf.random_normal([1]), name=\"bias2\")\n",
    "\n",
    "# Hypothesis\n",
    "logits = tf.matmul(layer1,W2) + b2\n",
    "H = tf.sigmoid(logits)\n",
    "\n",
    "# Cost Function\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "\n",
    "# train node\n",
    "# train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습\n",
    "for step in range(30000):\n",
    "    _, cost_val = sess.run([train,cost], feed_dict={X:x_data, Y:y_data})\n",
    "        \n",
    "    if step % 3000 == 0:\n",
    "        print(cost_val)     \n",
    "        \n",
    "# Accuracy 측정 \n",
    "predict = tf.cast(H > 0.5, dtype=tf.float32)\n",
    "correct = tf.equal(predict,Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "print(\"정확도 : {}\".format(sess.run([accuracy], feed_dict={X:x_data, Y:y_data})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "55000\n",
      "(55000, 784)\n",
      "(55000, 10)\n",
      "46.9638\n",
      "14.053117\n",
      "16.032127\n",
      "8.286947\n",
      "0.48941773\n",
      "0.18940124\n",
      "7.378783e-07\n",
      "0.0\n",
      "0.0\n",
      "0.061747823\n",
      "정확도 :[0.9583]\n"
     ]
    }
   ],
   "source": [
    "## NNIST (Neural Network)\n",
    "## tensorflow에 example로 포함된 MNIST예제를 NN으로 학습.(accuracy => 95%)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Data Loading \n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True) # 폴더를 만들고 데이터 저장 , 압축파일 파일 압축풀 필요없음\n",
    "\n",
    "# 데이터 확인\n",
    "print(mnist.train.num_examples) # 학습용 데이터의 개수 / 학습용 = train , 테스트용 = \n",
    "print(mnist.train.images.shape) # (55000, 784)\n",
    "                                #  28 X 28 이미지를 1차원 형태로 저장\n",
    "print(mnist.train.labels.shape)\n",
    "\n",
    "# plt.imshow(mnist.train.images[0].reshape(28,28), cmap=\"Greys\", interpolation=\"nearest\") # 1차원데이터를 2차원데이터로 변경\n",
    "# # 55000개의 그림중에 이미지를 선택해서 픽셀정보를 줘야한다.\n",
    "# plt.show()\n",
    "# print(mnist.train.labels[0])\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None, 784], dtype=tf.float32) \n",
    "Y = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W1 = tf.Variable(tf.random_normal([784,256]), name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([256]), name=\"bias1\")\n",
    "layer1 = tf.nn.relu(tf.matmul(X,W1) + b1) # sigmoid대신 relu사용 :  값이 희미해지기 때문에 relu사용 / sigmoid : 0과 1사이  / softmax : 확률값\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256,256]), name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([256]), name=\"bias2\")\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1,W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256,10]), name=\"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([10]), name=\"bias3\")\n",
    "\n",
    "# Hypothesis\n",
    "logits = tf.matmul(layer2,W3) + b3  \n",
    "H = tf.nn.softmax(logits)\n",
    "\n",
    "# Cost Function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "\n",
    "# train node 생성\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "# train = optimizer.minimize(cost)\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "# GradientDescentOptimizer 보다 효율 좋은 AdamOptimizer 사용  / learning_rate=0.01 => learning_rate=0.001\n",
    "\n",
    "\n",
    "# Session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\"\"\" \n",
    "사용하는 데이터의 크기가 상당히 크다.\n",
    "메모리의 문제때문에 한번에 불러올수 없다.\n",
    "데이터의 크기에 상관없이 학습하는 방식이 필요하다.!! => 잘라서 학습하는 방식으로 수행\n",
    "epoch : training data를 1번 학습시키는 것.\n",
    "\"\"\"\n",
    "# epoch 학습진행\n",
    "training_epoch = 30 # for 루프를 30만큼 돌린다는 의미\n",
    "batch_size = 100 #  55000개의 행을 다 읽어들이는게 아니라 100개의 행을 읽어서 반복학습!! (아중루프가 돔) / 얼마만큼의 사이즈로 불러드릴껀지\n",
    "\n",
    "for step in range(training_epoch): # 30 epoch 만큼 반복\n",
    "    num_of_iter = int(mnist.train.num_examples /  batch_size) # 550번 끊어 읽겠다는 의미\n",
    "    cost_val = 0\n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size) \n",
    "        _, cost_val = sess.run([train,cost], feed_dict={X:batch_x , Y:batch_y})\n",
    "        \n",
    "    if step % 3 == 0:\n",
    "        print(cost_val)     \n",
    "\n",
    "# 학습진행\n",
    "# for step in range(3000):\n",
    "#     _, cost_val = sess.run([train,cost], feed_dict={X:mnist.train.images, Y:mnist.train.labels})\n",
    "    \n",
    "#     if step % 300 == 0:\n",
    "#         print(cost_val)\n",
    "\n",
    "# Accuracy(정확도) 측정\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "result = sess.run([accuracy], feed_dict={X:mnist.test.images, Y:mnist.test.labels})\n",
    "\n",
    "print(\"정확도 :{}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "55000\n",
      "(55000, 784)\n",
      "(55000, 10)\n",
      "0.12680094\n",
      "0.011600091\n",
      "0.0038947864\n",
      "0.0022243808\n",
      "0.011003909\n",
      "0.0013692505\n",
      "0.010666206\n",
      "0.0016398019\n",
      "0.02676882\n",
      "5.0507202e-05\n",
      "정확도 :[0.978]\n"
     ]
    }
   ],
   "source": [
    "## NNIST (Neural Network)\n",
    "## tensorflow에 example로 포함된 MNIST예제를 NN으로 학습.(accuracy => 95%)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Data Loading \n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True) # 폴더를 만들고 데이터 저장 , 압축파일 파일 압축풀 필요없음\n",
    "\n",
    "# 데이터 확인\n",
    "print(mnist.train.num_examples) # 학습용 데이터의 개수 / 학습용 = train , 테스트용 = \n",
    "print(mnist.train.images.shape) # (55000, 784)\n",
    "                                #  28 X 28 이미지를 1차원 형태로 저장\n",
    "print(mnist.train.labels.shape)\n",
    "\n",
    "# plt.imshow(mnist.train.images[0].reshape(28,28), cmap=\"Greys\", interpolation=\"nearest\") # 1차원데이터를 2차원데이터로 변경\n",
    "# # 55000개의 그림중에 이미지를 선택해서 픽셀정보를 줘야한다.\n",
    "# plt.show()\n",
    "# print(mnist.train.labels[0])\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None, 784], dtype=tf.float32) \n",
    "Y = tf.placeholder(shape=[None, 10], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias # weight값 초기화\n",
    "W1 = tf.get_variable(\"weight1\", shape=[784,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]), name=\"bias1\")\n",
    "layer1 = tf.nn.relu(tf.matmul(X,W1) + b1) # sigmoid대신 relu사용 :  값이 희미해지기 때문에 relu사용 / sigmoid : 0과 1사이  / softmax : 확률값\n",
    "\n",
    "W2 = tf.get_variable(\"weight2\",shape=[256,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]), name=\"bias2\")\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1,W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"weight3\",shape=[256,10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]), name=\"bias3\")\n",
    "\n",
    "# Hypothesis\n",
    "logits = tf.matmul(layer2,W3) + b3  \n",
    "H = tf.nn.relu(logits)\n",
    "\n",
    "# Cost Function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "\n",
    "# train node 생성\n",
    "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "# train = optimizer.minimize(cost)\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "# GradientDescentOptimizer 보다 효율 좋은 AdamOptimizer 사용  / learning_rate=0.01 => learning_rate=0.001\n",
    "\n",
    "\n",
    "# Session & 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\"\"\" \n",
    "사용하는 데이터의 크기가 상당히 크다.\n",
    "메모리의 문제때문에 한번에 불러올수 없다.\n",
    "데이터의 크기에 상관없이 학습하는 방식이 필요하다.!! => 잘라서 학습하는 방식으로 수행\n",
    "epoch : training data를 1번 학습시키는 것.\n",
    "\"\"\"\n",
    "# epoch 학습진행\n",
    "training_epoch = 30 # for 루프를 30만큼 돌린다는 의미\n",
    "batch_size = 100 #  55000개의 행을 다 읽어들이는게 아니라 100개의 행을 읽어서 반복학습!! (아중루프가 돔) / 얼마만큼의 사이즈로 불러드릴껀지\n",
    "\n",
    "for step in range(training_epoch): # 30 epoch 만큼 반복\n",
    "    num_of_iter = int(mnist.train.num_examples /  batch_size) # 550번 끊어 읽겠다는 의미\n",
    "    cost_val = 0\n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size) \n",
    "        _, cost_val = sess.run([train,cost], feed_dict={X:batch_x , Y:batch_y})\n",
    "        \n",
    "    if step % 3 == 0:\n",
    "        print(cost_val)     \n",
    "\n",
    "# 학습진행\n",
    "# for step in range(3000):\n",
    "#     _, cost_val = sess.run([train,cost], feed_dict={X:mnist.train.images, Y:mnist.train.labels})\n",
    "    \n",
    "#     if step % 300 == 0:\n",
    "#         print(cost_val)\n",
    "\n",
    "# Accuracy(정확도) 측정\n",
    "predict = tf.argmax(H,1)\n",
    "correct = tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "result = sess.run([accuracy], feed_dict={X:mnist.test.images, Y:mnist.test.labels})\n",
    "\n",
    "print(\"정확도 :{}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3, 1)\n",
      "(2, 2, 1, 3)\n",
      "(1, 2, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "## 2019-07-17\n",
    "# Convolution example\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# imange의 형태\n",
    "# 1장의 이미지는 3차원 형태의 데이터\n",
    "# (이미지의 개수, width, height, color)\n",
    "# (1, 3, 3, 1)\n",
    "image = np.array([[[[1],[2],[3]],\n",
    "                  [[4],[5],[6]],\n",
    "                  [[7],[8],[9]]]], dtype=np.float32)\n",
    "print(image.shape)\n",
    "# filter를 준비해야한다. => 3차원 \n",
    "# (width, height, color, 필터의 개수) , 4차원\n",
    "# (2, 2, 1, 1)\n",
    "# 필터들의 집합 => weight\n",
    "weight = np.array([[[[1, -5, 10]],[[1, -5, 10]]],\n",
    "                   [[[1, -5, 10]],[[1, -5, 10]]]])\n",
    "print(weight.shape)\n",
    "\n",
    "# stride 지정 => 사실 2차원이면 되는데 행렬연산때문에 \n",
    "# (1, stride width, stride height, 1) => 4차원으로 표현\n",
    "# stride = [1,1,1,1]\n",
    "#                    (이미지,필터,)\n",
    "conv2d = tf.nn.conv2d(image,weight,strides=[1,1,1,1], padding=\"VALID\") \n",
    "# padding=\"VALID\": 이미지의 사이즈가 줄어든다. / padding=\"SAME\" :원본과 똑같이 맞추라는 의미\n",
    "print(conv2d.shape)\n",
    "# 하나의 이미지에 필터 3개를 적용시키면 (1, 2, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 14, 14, 5)\n",
      "(5, 14, 14, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## mnist 예제를 이용해서 하나의 이미지에 대한\n",
    "## convolutional image 5개를 생성해보기!\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt # 일반적으로 그림그리는 모듈\n",
    "\n",
    "# Data Loading\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True) # y측 레이블이 one_hot형태로 나올수 있게 해줌\n",
    "\n",
    "# training 이미지 중 2번째 이미지의 정보를 얻어온다. 픽셀테이터 땡겨오는것\n",
    "img = mnist.train.images[1] # 1차원 데이터\n",
    "img = img.reshape(28,28)    # 2차원 데이터로 변환\n",
    "# print(img.shape) # 1차원의형태를 2차원의 형태로 변경해줘야한다.\n",
    "\n",
    "plt.imshow(img, cmap=\"Greys\", interpolation=\"nearest\")\n",
    "plt.show()\n",
    "\n",
    "# 해당 이미지를 convolution 이미지로 변경\n",
    "# 2차원 형태의 img를 4차원 형태의 img로 변환\n",
    "img = img.reshape(-1,28,28,1) # 가로,세로는 변하지 않고 흑백컬러1 / 이미지가 여러장이면 -1부분이 바뀌게 될것임.\n",
    "# img.shape\n",
    "# 이미지가 준비 되었으니 필터를 여러개 준비 (5개 정도)\n",
    "# 5개의 필터를 이용 , 2 x 2 짜리 필터를 이용\n",
    "# (2,2,1,5) 랜덤하게 만들꺼임\n",
    "W = tf.Variable(tf.random_normal([2,2,1,5]), name=\"filter\")\n",
    "conv2d = tf.nn.conv2d(img,W,strides=[1,2,2,1], padding=\"SAME\")\n",
    "# 2칸씩움직이는 stride / padding=\"SAME\" : 원본과 똑같은 이미지가 5개 생성\n",
    "print(conv2d.shape) # (1, 14, 14, 5) => 14 x 14 짜리 이미지가 5개 생성\n",
    "\n",
    "# 새로 생성된 이미지를 plt를 이용해서 확인\n",
    "sess = tf.Session() # sess.run을 해야지 배열값을 얻을수 있다.\n",
    "sess.run(tf.global_variables_initializer()) # 초기화\n",
    "conv2d = sess.run(conv2d)\n",
    "\n",
    "# 배열의 축을 임의로 변경 (1, 14, 14, 5) => (5, 14, 14, 1) -> 루프를 돌리기 위해서 배열 축 임의로 변경 , 데이터를 쉽게 추출하기 위해서\n",
    "conv2d = np.swapaxes(conv2d,0,3) \n",
    "print(conv2d.shape) # (5, 14, 14, 1)\n",
    "\n",
    "fig,axes = plt.subplots(1,5) # 1행 5열짜리 subplot을 생성 / subplots : 그림을 여러개 그릴 수 있음\n",
    "                             # axes : 각각의 subplot의 배열\n",
    "for idx,item in enumerate(conv2d): # enumerate : 인덱스 번호와 컬렉션의 원소를 tuple형태로 반환\n",
    "    axes[idx].imshow(item.reshape(14,14), cmap=\"Greys\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "(?, 14, 14, 32)\n",
      "(?, 7, 7, 32)\n",
      "(?, 7, 7, 64)\n",
      "(?, 7, 7, 64)\n",
      "Wall time: 1.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## 셀 수행하는 데 얼마나 걸리는지 출력하는거, 제일 상단에 있어야한다. %%time은 jupyter notebook에서만 돌아간당!!!\n",
    "#### MNIST with CNN\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "## 0. 그래프 초기화\n",
    "tf.reset_default_graph()\n",
    "\n",
    "## 1. Data Loading & Data 정제\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "\n",
    "## 2. placeholder\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32) \n",
    "Y = tf.placeholder(shape=[None,10], dtype=tf.float32) \n",
    "drop_rate = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "## 3. Convolution \n",
    "## 3.1 Convolution layer 1\n",
    "x_img =  tf.reshape(X,[-1,28,28,1]) # x에대한 shape를 변경해줄꺼임 2차원 -> 4차원 (이미지의 개수, 가로, 세로, 컬러)\n",
    "W1 = tf.Variable(tf.random_normal([2,2,1,32]), name=\"filter1\") # 2행 2열 색 32개의 필터\n",
    "L1 = tf.nn.conv2d(x_img,W1,strides=[1,2,2,1], padding=\"SAME\") # tf.nn.conv2d(입력이미지, 필터, stride=[],padding=\"\")\n",
    "                                                              #                               얼마만큼 움직일껀지\n",
    "print(L1.shape)  # (?, 14, 14, 32)\n",
    "\n",
    "# sigmoid는 할수록 값이 작아져서 0에 가까워지기 때문에 relu를 사용한다.\n",
    "L1 = tf.nn.relu(L1)\n",
    "\n",
    "L1 = tf.nn.max_pool(L1,ksize=[1,2,2,1], strides=[1,2,2,1], padding=\"SAME\") \n",
    "# tf.nn.max_pool(입력받을값,ksize=[],strides=[], padding=\"\") : 큰 데이터를 작은데이터로 줄이는 것\n",
    "print(L1.shape)  # (?, 7, 7, 32)\n",
    "\n",
    "## 3.2 Convolution layer 2\n",
    "W2 = tf.Variable(tf.random_normal([3,3,32,64]), name=\"filter2\") # 3행 3열 색 64개의 필터 => 32 -> 입력으로 들어오는 형태 shape를 맞춰줘야함\n",
    "L2 = tf.nn.conv2d(L1,W2,strides=[1,1,1,1], padding=\"SAME\") # tf.nn.conv2d(입력이미지, 필터, stride=[],padding=\"\")\n",
    "                                                              #                               얼마만큼 움직일껀지\n",
    "print(L2.shape)  # (?, 7, 7, 64)\n",
    "\n",
    "# sigmoid는 할수록 값이 작아져서 0에 가까워지기 때문에 relu를 사용한다.\n",
    "L2 = tf.nn.relu(L2)\n",
    "\n",
    "L2 = tf.nn.max_pool(L2,ksize=[1,1,1,1], strides=[1,1,1,1], padding=\"SAME\") \n",
    "# tf.nn.max_pool(입력받을값,ksize=[],strides=[], padding=\"\") : 큰 데이터를 작은데이터로 줄이는 것\n",
    "print(L2.shape)  # (?, 7, 7, 64) => ?는 이미지의 수\n",
    "\n",
    "L2 = tf.reshape(L2,[-1,7*7*64])\n",
    "\n",
    "## 4. Neural Network\n",
    "## 4.1 Weight & bias\n",
    "W3 = tf.get_variable(\"weight3\", shape=[7*7*64,256], initializer=tf.contrib.layers.xavier_initializer()) # shape=[\"컬럼수\",\"아웃풋수\"] / 위에서는 필터의 형태로 잡기위해 tf.Variable()사용\n",
    "\n",
    "# b3 = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpu_env] *",
   "language": "python",
   "name": "conda-env-cpu_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
