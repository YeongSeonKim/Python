{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "## 2019-07-19\n",
    "# Class \n",
    "# 1. 객체모델의 수단\n",
    "# 2. ADT ( abstract data type ) - 추상적 데이터 타입\n",
    "\n",
    "# Class안넹 정의 되는 내용 3가지\n",
    "# 상태값 저장을 위한 변수 (field, member Variable, property)\n",
    "# 수행하는 작업을 위한 함수 (method, member function, method)\n",
    "# 정의된 class 정보를 바탕으로 일정 메모리 확보\n",
    "# => 이런 확보된 메모리 공간 = 인스턴스, 객체\n",
    "# 이런 객체를 생성하기 위해 생성자가 호출되어야함\n",
    "\n",
    "# 학생 3명\n",
    "# 각 학생이 가지는 정보는 국어, 영어, 수학 점수\n",
    "# 평균, 총점 구하기\n",
    "\n",
    "class Student:\n",
    "    # Constructor 생성자\n",
    "    def __init__(self,s_name,s_kor,s_eng,s_math):\n",
    "        # property를 생성하려면 self를 이용\n",
    "        self.student_name = s_name\n",
    "        self.kor = s_kor\n",
    "        self.eng = s_eng\n",
    "        self.math = s_math\n",
    "        \n",
    "    # method\n",
    "    def get_total(self):\n",
    "        self.total = self.kor + self.eng + self.math\n",
    "        return self.total\n",
    "    \n",
    "stu1 = Student(\"홍길동\",10,20,30)\n",
    "print(stu1.get_total())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-e3f974a0a1f2>:122: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-1-e3f974a0a1f2>:38: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-1-e3f974a0a1f2>:40: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.max_pooling2d instead.\n",
      "WARNING:tensorflow:From <ipython-input-1-e3f974a0a1f2>:53: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "Model0 학습중, epoch: 10, cost: 0.12258072197437286\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_accuracy() missing 1 required positional argument: 'drop_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e3f974a0a1f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: get_accuracy() missing 1 required positional argument: 'drop_rate'"
     ]
    }
   ],
   "source": [
    "## Ensemble을 이용한 MNIST\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "## Graph 초기화\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 모델을 여러개 만들어야함 => model class를 만든다.\n",
    "## Class Define\n",
    "\n",
    "## 1. Modeling\n",
    "class CnnModel:\n",
    "    # 생성자(Constructor)가 나와야함 \n",
    "    # def = define\n",
    "    def __init__(self,sess,name): # model이름을 지정함 / sess: 나중에 학습(run)하고 Accuracy구할때 사용하기 위해 지정\n",
    "        # property를 생성하려면 self를 이용 , self를 붙이면 전역변수 안붙이면 지역변수\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        self.build_net()\n",
    "        \n",
    "    # Model graph를 생성하는 method\n",
    "    def build_net(self):\n",
    "        # pass :  코드가 없음을 의미 , 자바에서는 {} 이용\n",
    "        \n",
    "        # 입력 받은 이름으로 변수 명을 설정한다.\n",
    "        with tf.variable_scope(self.name):\n",
    "            \n",
    "        # input place holders\n",
    "            self.X = tf.placeholder(shape=[None,784], dtype=tf.float32) \n",
    "            self.Y = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "            self.drop_rate = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "            # Convolution  \n",
    "            # 28x28x1로 사이즈 변환 \n",
    "            X_img = tf.reshape(self.X,[-1,28,28,1])\n",
    "\n",
    "            # Convolution Layer1 \n",
    "            L1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3,3], padding=\"SAME\", strides=1, activation=tf.nn.relu) \n",
    "            # Pooling Layer1 \n",
    "            L1 = tf.layers.max_pooling2d(inputs=L1, pool_size=[2,2], padding=\"SAME\", strides=2)\n",
    "\n",
    "            # Convolution Layer2\n",
    "            L2 = tf.layers.conv2d(inputs=L1, filters=64, kernel_size=[3,3], padding=\"SAME\", strides=1, activation=tf.nn.relu) \n",
    "            # Pooling Layer2 \n",
    "            L2 = tf.layers.max_pooling2d(inputs=L2, pool_size=[2,2], padding=\"SAME\", strides=2)\n",
    "            # Neural Network => 2차원을 4차원으로\n",
    "            L2 = tf.reshape(L2,[-1,7*7*64])\n",
    "\n",
    "            # Weight & bias \n",
    "            W1 = tf.get_variable(\"weight1\", shape=[7*7*64, 256], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "            b1 = tf.Variable(tf.random_normal([256]), name=\"bias1\")\n",
    "            _layer1 = tf.nn.relu(tf.matmul(L2,W1) +b1)\n",
    "            layer1 = tf.layers.dropout(_layer1, rate=self.drop_rate)\n",
    "\n",
    "            W2 = tf.get_variable(\"weight2\", shape=[256, 10], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "            b2 = tf.Variable(tf.random_normal([10]), name=\"bias2\")\n",
    "#             W2 =  tf.get_variable(\"weight2\", shape=[256,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "#             b2 = tf.Variable(tf.random_normal([256]), name=\"bias2\")\n",
    "#             _layer2 = tf.nn.relu(tf.matmul(layer1,W2) + b2)\n",
    "#             layer2 = tf.nn.dropout(_layer2, keep_prob = keep_prob)\n",
    "\n",
    "#             W3 = tf.get_variable(\"weight3\", shape=[7*7*64,256], initializer=tf.contrib.layers.xavier_initializer()) \n",
    "#             b3 = tf.Variable(tf.random_normal([]), name=\"bias3\")\n",
    "\n",
    "            # Hypothesis\n",
    "            logits = tf.matmul(layer1,W2) + b2  \n",
    "            self.H = tf.nn.relu(logits)\n",
    "\n",
    "            # Cost Function\n",
    "            self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = self.Y))\n",
    "\n",
    "            # train\n",
    "            self.train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(self.cost)\n",
    "            \n",
    "    ## Model 학습\n",
    "    def train_net(self,train_x_data, train_y_data, drop_rate): # 학습을 하려면 데이터가 필요 / rate : drop_out에 대한 비율\n",
    "        # pass\n",
    "        training_epoch = 10\n",
    "        batch_size = 30 \n",
    "        \n",
    "        for step in range(training_epoch): \n",
    "            num_of_iter = int( train_x_data.shape[0] /  batch_size) \n",
    "            cost_val = 0\n",
    "            # (x데이터,y데이터)\n",
    "            start_idx = 0\n",
    "            end_idx = 0\n",
    "        for i in range(num_of_iter):\n",
    "            # batch_x, batch_y = mnist.train.next_batch(batch_size) \n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            # if end_idx > :\n",
    "            # end_idx = batch_size\n",
    "        \n",
    "            batch_x = train_x_data[start_idx:end_idx,:]\n",
    "            batch_y = train_y_data[start_idx:end_idx,:]\n",
    "        \n",
    "            _, cost_val = self.sess.run([self.train,self.cost], feed_dict={self.X:batch_x , self.Y:batch_y, self.drop_rate:drop_rate})\n",
    "        \n",
    "        print(\"{0} 학습중, epoch: {1}, cost: {2}\".format(self.name, training_epoch, cost_val))\n",
    "\n",
    "    \n",
    "    ## Model의 정확도 측정\n",
    "    def get_accuracy(self,test_x_data, test_y_data, drop_rate): # 학습한 데이터에 대한 test data 필요\n",
    "        # pass\n",
    "        self.predict = tf.argmax(self.H,1)\n",
    "        correct = tf.equal(self.predict, tf.argmax(self.Y,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "\n",
    "        result = self.sess.run(self.accuracy, feed_dict={self.X:test_x_data, self.Y:test_y_data, self.drop_rate:drop_rate})\n",
    "\n",
    "        print(\"{0}의 Accuracy: {1}\".format(self.name, result))\n",
    "    \n",
    "    ## Model의 Prediction\n",
    "    def get_prediction(self, x_data, drop_out_rate):\n",
    "        # pass\n",
    "        result = self.sess.run(self.predict, feed_dict={self.X: x_data, self.drop_rate: drop_out_rate})\n",
    "\n",
    "    \n",
    "#######################  여기까지는 그래프를 그리고 학습을 하고 정확도 측정, prediction\n",
    "    \n",
    "## 2. Data Loading\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "\n",
    "# Session 생성\n",
    "sess = tf.Session()\n",
    "\n",
    "## 3. Model의 개수를 지정하고 생성 = Ensemble\n",
    "num_of_model = 10\n",
    "cnn_models = [CnnModel(sess,\"Model\" + str(x)) for x in range(num_of_model)] # 10번돌면서 [모델(세션, 모델의 이름) for 루프] \n",
    "\n",
    "for model in cnn_models:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    model.train_net(mnist.train.images, mnist.train.labels,0.3)\n",
    "    model.get_accuracy(mnist.test, 0.3)\n",
    "    model.get_prediction(mnist.test.images, 0.3)\n",
    "    \n",
    "## 4. 각각의 Model 학습\n",
    "\n",
    "## 5. 각 Model의 accuracy 출력\n",
    "\n",
    "## 6. ensemble의 accuracy 출력\n",
    "\n",
    "## 7. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 앙상블(Ensemble) => 모델을 여러개 만들어서 코드처리해야함. / 정확도는 증가하지만 속도는 느려짐(모델이 여러개 이여서)\n",
    "### 앙상블은 이런 model이 여러개 있다.\n",
    "### H1 => [0.5,,0.8, 0.99, 0.12, 0.34, ...]\n",
    "### H2 => [0.2,,0.3, 0.94, 0.5, 0.1, ...]\n",
    "### H3 => [0.7,,0.1, 0.3, 0.2, 0.12, ...]\n",
    "### H4 => [0.26,,0.23, 0.194, 0.56, 0.31, ...]\n",
    "\n",
    "### SUM => [1.66, 1.43, 2.4, 1.3, 1.2, ...] 컬럼별로 값들을 더함\n",
    "### 최종 prediction은 SUM한 결과값을 가지고 예측 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ensamble 을 이용한 MNIST\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "##Graph 초기화\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "\n",
    "##Class Define\n",
    "class CnnModel:\n",
    "    #Constructor\n",
    "    def __init__(self,sess,name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        self.build_net()\n",
    "      \n",
    "    #Model graph 생성하는 method\n",
    "    def build_net(self):\n",
    "       # 입력 받은 이름으로 변수 명을 설정한다.\n",
    "        with tf.variable_scope(self.name):\n",
    "\n",
    "            # Boolean Tensor 생성 for dropout\n",
    "            # tf.layers.dropout( training= True/Fals) True/False에 따라서 학습인지 / 예측인지 선택하게 됨\n",
    "            # default = False\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "\n",
    "            # 입력 그래프 생성\n",
    "            self.X = tf.placeholder(tf.float32, [None, 784])\n",
    "            # 28x28x1로 사이즈 변환\n",
    "            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "            # Convolutional Layer1\n",
    "            conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3,3], padding='SAME', \n",
    "                                     activation=tf.nn.relu)\n",
    "            # Pooling Layer1\n",
    "            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2,2], strides=2,\n",
    "                                            padding=\"SAME\" )\n",
    "            # Dropout Layer1\n",
    "            dropout1 = tf.layers.dropout(inputs=pool1, rate=0.7, training=self.training)\n",
    "\n",
    "\n",
    "            # Convolutional Layer2\n",
    "            conv2 = tf.layers.conv2d(inputs=dropout1, filters=64, kernel_size=[3,3], padding='SAME', activation=tf.nn.relu)\n",
    "            # Pooling Layer2\n",
    "            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2,2],strides=2, padding='SAME' )\n",
    "            # Dropout Layer2\n",
    "            dropout2 = tf.layers.dropout(inputs=pool2, rate=0.7, training=self.training)\n",
    "\n",
    "\n",
    "            # Convolutional Layer3\n",
    "            conv3 = tf.layers.conv2d(inputs=dropout2, filters=128, kernel_size=[3, 3], padding='SAME', activation=tf.nn.relu)\n",
    "            # Pooling Layer3\n",
    "            pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2,2], strides=2, padding='SAME')\n",
    "            # Dropout Layer3\n",
    "            dropout3 = tf.layers.dropout(inputs=pool3, rate=0.7, training=self.training)\n",
    "\n",
    "            # Dense Layer4 with Relu\n",
    "            flat = tf.reshape(dropout3, [-1, 128*4*4])\n",
    "            dense4 = tf.layers.dense(inputs=flat, units=625, activation=tf.nn.relu)\n",
    "            # Dropout layer4\n",
    "            dropout4 = tf.layers.dropout(inputs=dense4, rate=0.5, training=self.training)\n",
    "\n",
    "            # Dense Layer5 with Relu\n",
    "            dense5 = tf.layers.dense(inputs=dropout4, units=1050, activation=tf.nn.relu)\n",
    "            # Dropout Layer5\n",
    "            dropout5 = tf.layers.dropout(inputs=dense5, rate=0.5, training=self.training)\n",
    "\n",
    "\n",
    "            # Logits layer : Final FC Layer5 Shape = (?, 625) -> 10\n",
    "            self.logits = tf.layers.dense(inputs=dropout5, units=10)\n",
    "\n",
    "        # Cost Function\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.Y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.cost)\n",
    "\n",
    "        # Test Model\n",
    "        correct_prediction = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    #Model 학습\n",
    "    def train_net(self,train_x_data,train_y_data,training=False):\n",
    "        return self.sess.run([self.cost, self.optimizer], \n",
    "                             feed_dict={self.X:train_x_data, \n",
    "                                        self.Y:train_y_data,\n",
    "                                        self.training:training})\n",
    "     \n",
    "   #Model의 Accuracy측정\n",
    "    def get_accuracy(self,test_x_data,test_y_data,training=False):\n",
    "        return self.sess.run(self.accuracy, \n",
    "                             feed_dict={self.X: test_x_data, \n",
    "                                        self.Y : test_y_data, \n",
    "                                        self.training:training})\n",
    "    \n",
    "   #Model의 Prediction\n",
    "    def get_prediction(self,x_data,training=False):\n",
    "        return self.sess.run(self.logits, feed_dict={self.X : x_data, self.training:training})\n",
    "    \n",
    "\n",
    "\n",
    "sess=tf.Session()\n",
    "##1.Data Loading\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\",\n",
    "                                  one_hot=True)\n",
    "\n",
    "# Hyper Prarameters\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "##2. Model 개수를 지정하고 생성\n",
    "models = []\n",
    "num_of_model = 2\n",
    "#cnn_models = [CnnModel(sess,\"Model\" + str(x)) for x in range(num_of_model)]\n",
    "for m in range(num_of_model):\n",
    "    models.append(CnnModel(sess, \"model\"+str(m)))\n",
    "                                                \n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost_list = np.zeros(len(cnn_models))\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        # Train each model\n",
    "        for m_idx, m in enumerate(cnn_models):\n",
    "            c, _ = m.train_net(batch_xs, batch_ys)\n",
    "            avg_cost_list[m_idx] += c / total_batch\n",
    "\n",
    "    print('Epoch: ', '%04d' %(epoch + 1), 'Cost = ', avg_cost_list)\n",
    "print('Training Finished')\n",
    "\n",
    "                                                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpu_env] *",
   "language": "python",
   "name": "conda-env-cpu_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
