{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-f4a421e0bf1b>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-1-f4a421e0bf1b>:42: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\cpu_env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-1-f4a421e0bf1b>:45: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.max_pooling2d instead.\n",
      "(?, 7, 7, 64)\n",
      "WARNING:tensorflow:From <ipython-input-1-f4a421e0bf1b>:67: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "0.070452996\n",
      "0.06277607\n"
     ]
    }
   ],
   "source": [
    "## Mini Team Project\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#1. data loading\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "\n",
    "#2. model 정의(Tensorflow graph생성 ,데이터를 학습하기 위한 model)\n",
    "tf.reset_default_graph() #Tensorflow graph 초기화\n",
    "#2.1 placeholder\n",
    "#shape는 입력데이터에 따라 결정됨\n",
    "X = tf.placeholder(shape=[None,784], dtype=tf.float32,name=\"input\") #\n",
    "Y = tf.placeholder(shape=[None,10], dtype=tf.float32,name=\"output\")\n",
    "drop_rate =tf.placeholder(dtype=tf.float32,name=\"drop\") #스칼라는 1차원이기때문에 shape없이 생성\n",
    "#2.2 Convolution 작업\n",
    "#CNN은 이미지 학습에 최적화된 deep learning방법\n",
    "#입력받은 이미지의 형태가 4차원 매트릭스\n",
    "#X데이터가 2차원이기때문에 4차원 매트릭스로 변경\n",
    "#(이미지 개수, 이미지의 width, 이미지의 height,color수)\n",
    "#color수 - 1 흑백, 3 컬러\n",
    "X_img = tf.reshape(X,[-1,28,28,1])\n",
    "#Convolution Layer1\n",
    "# #필터정의->filter의 shape(width,height,color,filter수)\n",
    "# #(데이터가 4차원이라서 필터도 4차원으로 생성)\n",
    "# filter1 = tf.Variable(tf.random_normal([3,3,1,32]))\n",
    "# #filter를 이용해서 convolution image를 생성\n",
    "# #행렬곱 처리\n",
    "# #strides=[] 맨앞1,맨뒤1 더미데이터\n",
    "# L1 = tf.nn.conv2d(X_img,filter,strides=[1,1,1,1],\n",
    "#                  padding=\"SAME\")\n",
    "# #만들어진 convolution에 relu를 적용\n",
    "# #(NN에서 sigmoid경우 값이 흐려지기 때문에(값이 0) relu사용)\n",
    "# L1 = tf.nn.relu(L1)\n",
    "# # pooling작업(resize, sampling 작업) = > optional 선택사항(데이터 사이즈가 클경우 사용)\n",
    "# #strides=[],ksize=[] 맨앞1,맨뒤1 더미데이터\n",
    "# L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1],\n",
    "#                     strides=[1,2,2,1],padding=\"SAME\")\n",
    "\n",
    "#Convolution Layer1 생성 => tf.layers.conv2d()으로 대체\n",
    "#kernel_size=[3,3] - filter size 가로*세로\n",
    "L1 = tf.layers.conv2d(inputs=X_img, filters=32,\n",
    "                     kernel_size=[3,3], padding=\"SAME\", \n",
    "                      strides=1,activation=tf.nn.relu)\n",
    "#tf.nn.max_pool() => tf.layers.max_pooling2d() 대체\n",
    "L1 = tf.layers.max_pooling2d(inputs=L1,pool_size=[2,2],\n",
    "                            padding=\"SAME\",strides=2)\n",
    "#Convolution Layer2\n",
    "L2 = tf.layers.conv2d(inputs=L1, filters=64,\n",
    "                     kernel_size=[3,3], padding=\"SAME\", \n",
    "                      strides=1,activation=tf.nn.relu)\n",
    "\n",
    "L2 = tf.layers.max_pooling2d(inputs=L2,pool_size=[2,2],\n",
    "                            padding=\"SAME\",strides=2)\n",
    "\n",
    "print(L2.shape)\n",
    "#(?, 7, 7, 64) ?행 7*7 64개\n",
    "#2.3 Neural Network\n",
    "#Convolution의 결과(4차원)를 \n",
    "#Neural Network의 입력(2차원)으로 사용하기 위해 shape변경\n",
    "L2 = tf.reshape(L2,[-1,7*7*64])\n",
    "\n",
    "#Weight * bias \n",
    "#행렬곱을 연산을 위해 shape=[7*7*64,256] 지정, 256 logistic\n",
    "W1 = tf.get_variable(\"weight1\",shape=[7*7*64,256],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]), name=\"bias1\")\n",
    "_layer1 = tf.nn.relu(tf.matmul(L2,W1)+ b1)\n",
    "layer1 = tf.layers.dropout(_layer1, rate=drop_rate)\n",
    "\n",
    "W2 = tf.get_variable(\"weight2\",shape=[256,10],\n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([10]), name=\"bias2\")\n",
    "#Hypothesis\n",
    "logits = tf.matmul(layer1,W2) + b2\n",
    "H = tf.nn.relu(logits)\n",
    "H = tf.identity(H,name=\"hypothesis\")\n",
    "\n",
    "#cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=Y))\n",
    "\n",
    "#train\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "#session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "#학습 진행(batch처리)\n",
    "training_epoch = 10 \n",
    "batch_size = 100 \n",
    "\n",
    "for step in range(training_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples / batch_size)\n",
    "    cost_val = 0\n",
    "    for i in range(num_of_iter):\n",
    "        batch_x,batch_y = mnist.train.next_batch(batch_size) #지정한 사이즈만큼 데이터를 읽어옴\n",
    "        _, cost_val = sess.run([train,cost],feed_dict={X:batch_x,Y:batch_y,drop_rate:0.7})\n",
    "    \n",
    "    if step % 3 == 0:\n",
    "        print(cost_val)\n",
    "        \n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, \"./yeongseon/cnn/model/img_model.ckpt\")\n",
    "\n",
    "#Accuracy 측정\n",
    "predict = tf.argmax(H,1) #열방향으로 큰값 추출\n",
    "correct = tf.equal(predict, tf.argmax(Y,1)) #실제값과 예측값 비교\n",
    "accuracy = tf.reduce_mean(tf.cast(correct,dtype=tf.float32))\n",
    "\n",
    "result = sess.run(accuracy,feed_dict={X:mnist.test.images,\n",
    "                                     Y:mnist.test.labels})\n",
    "\n",
    "print(\"정확도:{}\".format(result))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpu_env] *",
   "language": "python",
   "name": "conda-env-cpu_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
